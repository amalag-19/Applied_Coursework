ggtitle("KDE for Barium")+geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba, size = 1, colour = "black")+
scale_x_continuous(limits = c(min(df_latlong_Ba$Longitude)-0.5, max(df_latlong_Ba$Longitude))+0.1, expand = c(0, 0))+scale_y_continuous(limits = c(min(df_latlong_Ba$Latitude)-0.5, max(df_latlong_Ba$Latitude))+0.1, expand = c(0, 0))+
theme(plot.title = element_text(hjust = 0.55,size = 20))
## 2D histogram
library(RColorBrewer)
rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(32)
ggmap(map_Ba, extent = "device")  +
stat_bin2d(aes(x = Longitude, y = Latitude), bins = 10,data = df_latlong_Ba) +
ggtitle("2D histogram for Barium")+
geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba[which(dfBa_ordered$conc<=63),], size = 1, colour = "black")+geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba[which(dfBa_ordered$conc>63),], size = 1, colour = "yellow")+
scale_x_continuous(limits = c(min(df_latlong_Ba$Longitude)-0.5, max(df_latlong_Ba$Longitude))+0.1, expand = c(0, 0))+scale_y_continuous(limits = c(min(df_latlong_Ba$Latitude)-0.5, max(df_latlong_Ba$Latitude))+0.1, expand = c(0, 0))+
theme(plot.title = element_text(hjust = 0.55,size = 20))+
scale_fill_gradient(colours=r)
## Outliers
ggmap(map_Ba, extent = "device")  + geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba, size = 1, colour = "black")
#########################################################################################
library(ggmap)
df_count_Ba$countcut<-cut(x = df_count_Ba$count,breaks = c(0,10,20,50,100,200),right = TRUE,labels = 1:5)
map_Ba<-get_googlemap(center = c(lon = mean(df_count_Ba$Longitude), lat = mean(df_count_Ba$Latitude)),scale = 2,zoom = 7,maptype = "terrain")
ggmap(map_Ba, extent = "device")+geom_point(mapping = aes(x = Longitude, y = Latitude,shape=countcut,colour=countcut), data = df_count_Ba, size = 3)+
ggtitle("Number of observations for different locations for Barium")+
theme(plot.title = element_text(hjust = 0.55,size = 20))+
scale_shape_manual(name = "Count Intervals",labels = c("1-10","10-20","20-50","50-100","100-200"),values = c(3,4,17,16,15))+
scale_colour_manual(name = "Count Intervals",labels = c("1-10","10-20","20-50","50-100","100-200"),values=c("red","green","orange","blue","black"))
#########################################################################################
## Time series segmentation on surface water data
#########################################################################################
## Defining a wrapper function that does the following tasks using the subfunctions defined in it:
## (1) Takes as input a dataframe, gets the start value of the time series
## (2) Converts all the times into hours from this start value
## (3) Gives an ordered time series as an output with last column giving the label for year interval
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
wrapper<-function(df){
## Defining a function to subset a dataframe over minimum of a given column
colmin.subsetter<-function(df,colname){
ids<-which(df[,colname]==min(df[,colname]))
df.subset<-df[ids,]
return(df.subset)
}
## Defining a function to get the starting hour of a time series given columns "Year", "Month",
## "Day", "AmPm", "Time"
ts.start<-function(df){
## subsetting the Ba dataset for largest county over minimum year
df.year.min<-colmin.subsetter(df=df,colname="Year")
## subsetting this subset over minimum month
df.month.min<-colmin.subsetter(df=df.year.min,colname="Month")
## subsetting this subset over minimum day
df.day.min<-colmin.subsetter(df=df.month.min,colname="Day")
start<-c(df.day.min$Year[1],df.day.min$Month[1],df.day.min$Day[1])
return(start)
}
## Defining functions to convert a time in "Year, Month, Day, AmPm and Time" into hours given
## starting value
ymd.to.d<-function(time){
mtod<-matrix(c(1:12,31,29,31,30,31,30,31,31,30,31,30,31),12,2)
month.id<-which(mtod[,1]==time[2])
if(month.id>1){
total.days<-sum(mtod[1:(month.id-1),2])+(time[3]-1)
}
else{total.days<-(time[3]-1)}
left.days<-365-total.days
return(c(total.days,left.days))
}
day.convert<-function(time, start){
year.diff<-time[1]-start[1]
if (year.diff==0){
days<-ymd.to.d(time)[1]-ymd.to.d(start)[1]
} else if(year.diff==1){
days<-ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==0){
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==1){
if((start[1]+1)%%4==0){
years.leap<-((year.diff-1)%/%4)+1
years.normal<-(year.diff-1)-years.leap
}
else{
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
}
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==2){
if(((start[1]+1)%%4==0)|((start[1]+2)%%4==0)){
years.leap<-((year.diff-1)%/%4)+1
years.normal<-(year.diff-1)-years.leap
}
else{
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
}
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==3){
if(((start[1]+1)%%4==0)|((start[1]+2)%%4==0)|((start[1]+3)%%4==0)){
years.leap<-((year.diff-1)%/%4)+1
years.normal<-(year.diff-1)-years.leap
}
else{
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
}
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
}
days_2<-floor(days/2)
days_3<-floor(days/3)
days_5<-floor(days/5)
days_10<-floor(days/10)
days_20<-floor(days/20)
days_30<-floor(days/30)
days_60<-floor(days/30)
days_90<-floor(days/30)
days_180<-floor(days/180)
days_360<-floor(days/360)
days_vec<-c(days,days_2,days_3,days_5,days_10,days_20,days_30,days_60,days_90,days_180)
return(days_vec)
}
## Defining a function to get time matrix from a dataframe
get.time<-function(df){
times<-matrix(NA_real_,nrow(df),3)
for (j in 1:nrow(df)){
times[j,1:3]<-c(df$Year[j],df$Month[j],df$Day[j])
}
return(times)
}
## Using the above defined functions to get the time series
df.start<-ts.start(df)
df.times.raw<-get.time(df)
df.times<-t(apply(X = df.times.raw,MARGIN = 1,FUN = function(x) day.convert(x,df.start)))
df.ts<-data.frame(cbind(df.times[,1],df.times,df$DataValue,df$Latitude,df$Longitude,df$yearcut))
df.ts.ordered<-df.ts[order(df.ts[,1]),]
rownames(df.ts.ordered)<-1:nrow(df.ts.ordered)
names(df.ts.ordered)<-c("time","time_1d","time_2d","time_3d","time_5d","time_10d","time_20d","time_30d","time_60d","time_90d","time_180d","conc","Latitude","Longitude","year.interval")
return(df.ts.ordered)
}
#########################################################################################
## Reading the censored datasets
dfBa_cens<-read.csv("/Users/Amal/Box Sync/PSU/Fall 2016/RA/Surface Water/data/Barium Data/Barium_filtered.csv")
dfSu_cens<-read.csv(file = "/Users/Amal/Box Sync/PSU/Fall 2016/RA/Surface Water/data/Sulfate Data/Sulphate_filtered.csv")
## Cutting the dataframe according into year intervals as group variable
dfBa_cens$yearcut<-as.factor(cut(dfBa_cens$Year, c(1963,1973,1987,1997,2007,2015),right = FALSE,labels = c(1:5)))
dfSu_cens$yearcut<-as.factor(cut(dfSu_cens$Year, c(1921,1941,1961,1983,2002,2015),right = FALSE,labels = c(1:5)))
## Removing the NA values
Ba_NA.ids<-which(is.na(dfBa_cens$yearcut))
Su_NA.ids<-which(is.na(dfSu_cens$yearcut))
## Ba_NA.ids is a zero length vector and therefore not updated
dfSu_cens<-dfSu_cens[-c(Su_NA.ids),]
rownames(dfSu_cens)<-NULL
rm(Ba_NA.ids,Su_NA.ids)
## splitting the dataframes for different sites for both Ba and Su and storing them in lists
dfBa_county<-split(dfBa_cens,dfBa_cens$County)
dfSu_county<-split(dfSu_cens,dfSu_cens$County)
## Checking the head of Ba and Su dataframes
head(dfBa_cens)
head(dfSu_cens)
## Checking the county columns
dfBa_cens$County
dfSu_cens$County
## checking the lists
str(dfBa_county,max.level = 1)
str(dfSu_county,max.level = 1)
## The first element of Ba and Su does not have a named county and therefore removing it.
dfBa_county[[1]]<-NULL
dfSu_county[[1]]<-NULL
## Number of counties in Ba and Su
NBa_counties<-length(dfBa_county)
NSu_counties<-length(dfSu_county)
NBa_counties
NSu_counties
## There are 40 and 67 counties in total for Ba and Su respectively. Combining all the dataframes again. Splitting according to time periods first and then splitting according to counties later
dfBa_combined<-do.call("rbind", dfBa_county)
dfSu_combined<-do.call("rbind", dfSu_county)
head(dfBa_combined)
latlong_Ba<-cbind(dfBa_combined$Latitude,dfBa_combined$Longitude)
write.csv(latlong_Ba,file = "/Users/Amal/Box Sync/PSU/Spring 2017/RA/Surface Water/latlong files for Tao/latlong_Ba.csv")
head(dfSu_combined)
latlong_Su<-cbind(dfSu_combined$Latitude,dfSu_combined$Longitude)
write.csv(latlong_Su,file = "/Users/Amal/Box Sync/PSU/Spring 2017/RA/Surface Water/latlong files for Tao/latlong_Su.csv")
## Pre-processing
dfBa_ordered<-wrapper(dfBa_combined)
dfSu_ordered<-wrapper(dfSu_combined)
latlong_Ba<-cbind(dfBa_ordered$Latitude,dfBa_ordered$Longitude)
latlong_Su<-cbind(dfSu_ordered$Latitude,dfSu_ordered$Longitude)
ids_na<-c()
for (i in 1:nrow(latlong_Su)){
if(prod(!is.na(latlong_Su[i,]))==0){
ids_na<-c(ids_na,i)
}
}
latlong_Su<-latlong_Su[-ids_na,]
dfSu_ordered<-dfSu_ordered[-ids_na,]
#########################################################################################
## Analysis
## choosing the number of clusters using scree plot of within cluster sum of squares
latlong_Ba<-cbind(dfBa_ordered$Latitude,dfBa_ordered$Longitude)
#write.csv(latlong_Ba,file = "/Users/Amal/Box Sync/PSU/Spring 2017/RA/Surface Water/latlong files for Tao/latlong_Ba_ordered.csv")
latlong_Su<-cbind(dfSu_ordered$Latitude,dfSu_ordered$Longitude)
#write.csv(latlong_Su,file = "/Users/Amal/Box Sync/PSU/Spring 2017/RA/Surface Water/latlong files for Tao/latlong_Su_ordered.csv")
## Defining a function to calculate the unique latitude longitude dataframe and number of samples for each unique id
count_cal<-function(latlong){
repeat_mat<-matrix(NA_integer_,nrow(latlong),2)
k<-1
for (i in 1:(nrow(latlong)-1)){
if(i==1){
for (j in (i+1):nrow(latlong)){
if(all(latlong[i,]==latlong[j,])==TRUE){
repeat_mat[k,]<-c(i,j)
k<-k+1
}
}
}else if(i>=2){
if(!(i%in%repeat_mat[,2])){
for (j in (i+1):nrow(latlong)){
if(all(latlong[i,]==latlong[j,])==TRUE){
repeat_mat[k,]<-c(i,j)
k<-k+1
}
}
}
}
}
nrow(repeat_mat)
ids_na<-which(is.na(repeat_mat[,1]))
repeat_mat<-repeat_mat[-ids_na,]
length(unique(repeat_mat[,2]))
length((repeat_mat[,2]))
latlong_unique<-latlong[-repeat_mat[,2],]
ids_unique<-setdiff(c(1:nrow(latlong)),repeat_mat[,2])
count<-rep(1,length(ids_unique))
for(i in 1:length(ids_unique)){
if(ids_unique[i]%in%repeat_mat[,1]){
count[i]<-count[i]+sum(repeat_mat[,1]==ids_unique[i])
}
}
df_count<-data.frame(latlong_unique,count)
names(df_count)<-c("Latitude","Longitude","count")
return(df_count)
}
df_count_Ba<-count_cal(latlong = latlong_Ba)
df_count_Su<-count_cal(latlong = latlong_Su)
#########################################################################################
## 2D histogram for surface water data
#########################################################################################
## Loading the required libraries
library(RColorBrewer)
library(ggmap)
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
#########################################################################################
## Defining a wrapper function that does the following tasks using the subfunctions defined in it:
## (1) Takes as input a dataframe, gets the start value of the time series
## (2) Converts all the times into hours from this start value
## (3) Gives an ordered time series as an output with last column giving the label for year interval
wrapper<-function(df){
## Defining a function to subset a dataframe over minimum of a given column
colmin.subsetter<-function(df,colname){
ids<-which(df[,colname]==min(df[,colname]))
df.subset<-df[ids,]
return(df.subset)
}
## Defining a function to get the starting hour of a time series given columns "Year", "Month",
## "Day", "AmPm", "Time"
ts.start<-function(df){
## subsetting the Ba dataset for largest county over minimum year
df.year.min<-colmin.subsetter(df=df,colname="Year")
## subsetting this subset over minimum month
df.month.min<-colmin.subsetter(df=df.year.min,colname="Month")
## subsetting this subset over minimum day
df.day.min<-colmin.subsetter(df=df.month.min,colname="Day")
start<-c(df.day.min$Year[1],df.day.min$Month[1],df.day.min$Day[1])
return(start)
}
## Defining functions to convert a time in "Year, Month, Day, AmPm and Time" into hours given
## starting value
ymd.to.d<-function(time){
mtod<-matrix(c(1:12,31,29,31,30,31,30,31,31,30,31,30,31),12,2)
month.id<-which(mtod[,1]==time[2])
if(month.id>1){
total.days<-sum(mtod[1:(month.id-1),2])+(time[3]-1)
}
else{total.days<-(time[3]-1)}
left.days<-365-total.days
return(c(total.days,left.days))
}
day.convert<-function(time, start){
year.diff<-time[1]-start[1]
if (year.diff==0){
days<-ymd.to.d(time)[1]-ymd.to.d(start)[1]
} else if(year.diff==1){
days<-ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==0){
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==1){
if((start[1]+1)%%4==0){
years.leap<-((year.diff-1)%/%4)+1
years.normal<-(year.diff-1)-years.leap
}
else{
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
}
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==2){
if(((start[1]+1)%%4==0)|((start[1]+2)%%4==0)){
years.leap<-((year.diff-1)%/%4)+1
years.normal<-(year.diff-1)-years.leap
}
else{
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
}
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
} else if((year.diff-1)%%4==3){
if(((start[1]+1)%%4==0)|((start[1]+2)%%4==0)|((start[1]+3)%%4==0)){
years.leap<-((year.diff-1)%/%4)+1
years.normal<-(year.diff-1)-years.leap
}
else{
years.leap<-(year.diff-1)%/%4
years.normal<-(year.diff-1)-years.leap
}
days<-(366*years.leap+365*years.normal)+ymd.to.d(time)[1]+ymd.to.d(start)[2]
}
return(days)
}
## Defining a function to get time matrix from a dataframe
get.time<-function(df){
times<-matrix(NA_real_,nrow(df),3)
for (j in 1:nrow(df)){
times[j,1:3]<-c(df$Year[j],df$Month[j],df$Day[j])
}
return(times)
}
## Using the above defined functions to get the time series
df.start<-ts.start(df)
df.times.raw<-get.time(df)
df.times<-apply(X = df.times.raw,MARGIN = 1,FUN = function(x) day.convert(x,df.start))
df.ts<-data.frame(cbind(df.times,df$DataValue,df$Latitude,df$Longitude,df$yearcut))
df.ts.ordered<-df.ts[order(df.ts[,1]),]
rownames(df.ts.ordered)<-1:nrow(df.ts.ordered)
names(df.ts.ordered)<-c("time","conc","Latitude","Longitude","year.interval")
return(df.ts.ordered)
}
#########################################################################################
## Reading the censored datasets
dfBa_cens<-read.csv(file = "Barium_filtered.csv")
dfSu_cens<-read.csv(file = "Sulphate_filtered.csv")
## Cutting the dataframe according into year intervals as group variable
dfBa_cens$yearcut<-as.factor(cut(dfBa_cens$Year, c(1963,1973,1987,1997,2007,2015),right = FALSE,labels = c(1:5)))
dfSu_cens$yearcut<-as.factor(cut(dfSu_cens$Year, c(1921,1941,1961,1983,2002,2015),right = FALSE,labels = c(1:5)))
## Removing the NA values
Ba_NA.ids<-which(is.na(dfBa_cens$yearcut))
Su_NA.ids<-which(is.na(dfSu_cens$yearcut))
## Ba_NA.ids is a zero length vector and therefore not updated
dfSu_cens<-dfSu_cens[-c(Su_NA.ids),]
rownames(dfSu_cens)<-NULL
rm(Ba_NA.ids,Su_NA.ids)
## splitting the dataframes for different sites for both Ba and Su and storing them in lists
dfBa_county<-split(dfBa_cens,dfBa_cens$County)
dfSu_county<-split(dfSu_cens,dfSu_cens$County)
## Checking the head of Ba and Su dataframes
head(dfBa_cens)
head(dfSu_cens)
## Checking the county columns
dfBa_cens$County
dfSu_cens$County
## checking the lists
str(dfBa_county,max.level = 1)
str(dfSu_county,max.level = 1)
## The first element of Ba and Su does not have a named county and therefore removing it.
dfBa_county[[1]]<-NULL
dfSu_county[[1]]<-NULL
## Number of counties in Ba and Su
NBa_counties<-length(dfBa_county)
NSu_counties<-length(dfSu_county)
NBa_counties
NSu_counties
## There are 40 and 67 counties in total for Ba and Su respectively. Combining all the dataframes again. Splitting according to time periods first and then splitting according to counties later
dfBa_combined<-do.call("rbind", dfBa_county)
dfSu_combined<-do.call("rbind", dfSu_county)
head(dfBa_combined)
head(dfSu_combined)
## Pre-processing
#debug(wrapper)
dfBa_ordered<-wrapper(dfBa_combined)
dfSu_ordered<-wrapper(dfSu_combined)
latlong_Ba<-cbind(dfBa_ordered$Latitude,dfBa_ordered$Longitude)
latlong_Su<-cbind(dfSu_ordered$Latitude,dfSu_ordered$Longitude)
ids_na<-c()
for (i in 1:nrow(latlong_Su)){
if(prod(!is.na(latlong_Su[i,]))==0){
ids_na<-c(ids_na,i)
}
}
latlong_Su<-latlong_Su[-ids_na,]
dfSu_ordered<-dfSu_ordered[-ids_na,]
#########################################################################################
plotter<-function(analyte,visual,bins,threshold_Ba=NA,threshold_Su=NA){
if(analyte=="Barium"){
df_latlong_Ba<-data.frame(latlong_Ba)
names(df_latlong_Ba)<-c("Latitude","Longitude")
map_Ba<-get_googlemap(center = c(lon = mean(df_latlong_Ba$Longitude), lat = mean(df_latlong_Ba$Latitude)),scale = 2,zoom = 7,maptype = "terrain")
if(visual=="kde"){
p1<-ggmap(map_Ba, extent = "device")  +
stat_density2d(aes(x = Longitude, y = Latitude, fill = ..level..,alpha=..level..), bins = bins, geom = "polygon", data = df_latlong_Ba) +
scale_fill_gradient(low = "black", high = "red")+
ggtitle("KDE for Barium")+
geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba[which(dfBa_ordered$conc<=threshold_Ba),], size = 1, colour = "black")+geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba[which(dfBa_ordered$conc>threshold_Ba),], size = 1, colour = "yellow")+
scale_x_continuous(limits = c(min(df_latlong_Ba$Longitude)-0.5, max(df_latlong_Ba$Longitude))+0.1, expand = c(0, 0))+scale_y_continuous(limits = c(min(df_latlong_Ba$Latitude)-0.5, max(df_latlong_Ba$Latitude))+0.1, expand = c(0, 0))+
theme(plot.title = element_text(hjust = 0.55,size = 20))
}else if(visual=="hist"){
rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(32)
p1<-ggmap(map_Ba, extent = "device")  +
stat_bin2d(aes(x = Longitude, y = Latitude), bins = bins,data = df_latlong_Ba) +
ggtitle("2D histogram for Barium")+
geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba[which(dfBa_ordered$conc<=threshold_Ba),], size = 1, colour = "black")+geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Ba[which(dfBa_ordered$conc>threshold_Ba),], size = 1, colour = "yellow")+
scale_x_continuous(limits = c(min(df_latlong_Ba$Longitude)-0.5, max(df_latlong_Ba$Longitude))+0.1, expand = c(0, 0))+scale_y_continuous(limits = c(min(df_latlong_Ba$Latitude)-0.5, max(df_latlong_Ba$Latitude))+0.1, expand = c(0, 0))+
theme(plot.title = element_text(hjust = 0.55,size = 20))+
scale_fill_gradientn(colours=r)
}
}else if(analyte=="Sulphate"){
df_latlong_Su<-data.frame(latlong_Su)
names(df_latlong_Su)<-c("Latitude","Longitude")
map_Su<-get_googlemap(center = c(lon = mean(df_latlong_Su$Longitude), lat = mean(df_latlong_Su$Latitude)),scale = 2,zoom = 7,maptype = "terrain")
if(visual=="kde"){
p1<-ggmap(map_Su, extent = "device")  +
stat_density2d(aes(x = Longitude, y = Latitude, fill = ..level..,alpha=..level..), bins = bins, geom = "polygon", data = df_latlong_Su) +
scale_fill_gradient(low = "black", high = "red")+
ggtitle("KDE for Sulphate")+
geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Su[which(dfSu_ordered$conc<=threshold_Su),], size = 1, colour = "black")+geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Su[which(dfSu_ordered$conc>threshold_Su),], size = 1, colour = "yellow")+
scale_x_continuous(limits = c(min(df_latlong_Su$Longitude)-0.5, max(df_latlong_Su$Longitude))+0.1, expand = c(0, 0))+scale_y_continuous(limits = c(min(df_latlong_Su$Latitude)-0.5, max(df_latlong_Su$Latitude))+0.1, expand = c(0, 0))+
theme(plot.title = element_text(hjust = 0.55,size = 20))
}else if(visual=="hist"){
rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(32)
p1<-ggmap(map_Su, extent = "device")  +
stat_bin2d(aes(x = Longitude, y = Latitude), bins = bins,data = df_latlong_Su) +
ggtitle("2D histogram for Sulphate")+
geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Su[which(dfSu_ordered$conc<=threshold_Su),], size = 1, colour = "black")+geom_point(aes(x = Longitude, y = Latitude), data = df_latlong_Su[which(dfSu_ordered$conc>threshold_Su),], size = 1, colour = "yellow")+
scale_x_continuous(limits = c(min(df_latlong_Su$Longitude)-0.5, max(df_latlong_Su$Longitude))+0.1, expand = c(0, 0))+scale_y_continuous(limits = c(min(df_latlong_Su$Latitude)-0.5, max(df_latlong_Su$Latitude))+0.1, expand = c(0, 0))+
theme(plot.title = element_text(hjust = 0.55,size = 20))+
scale_fill_gradientn(colours=r)
}
}
return(p1)
}
plotter2<-function(analyte,seg_Ba=NA,seg_Su=NA){
if(analyte=="Barium"){
load(paste0("Ba_",as.numeric(seg_Ba),"d.RData"))
}else if(analyte=="Sulphate"){
remove(p1)
load(paste0("Su_",as.numeric(seg_Su),"d.RData"))
}
return(p1)
}
