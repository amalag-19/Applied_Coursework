{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\froman\fcharset0 Times-Roman;\f2\fnil\fcharset0 HelveticaNeue;
}
{\colortbl;\red255\green255\blue255;\red26\green26\blue26;\red255\green255\blue255;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww10800\viewh8920\viewkind0
\deftab720
\pard\pardeftab720\sa256

\f0\fs26 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 What you need to do for your peer reviews is the following:\
(1) 
\b \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Presentations
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 : In the link above, you will see two names next to your name. For each of these two names, you will have to provide a very brief review of their in-class presentations. Provide at least 1 comment about what you liked about the presentation and at least 1 suggestion for improvement. You are welcome to provide more feedback if you like; what I have written above is the minimum requirement.\
(2) 
\b \expnd0\expndtw0\kerning0
\outl0\strokewidth0 Writeups
\b0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 : In the link above, evaluate the project of just the first name next to your name. That is, you only need to evaluate one project. The comments should follow the rubric:\
(1) Clarity of writing: did you understand the main problem? Did you understand \'a0the conclusions? What was unclear?\
(2) Correctness of work: does the work seem correct? Are there any problems/issues with the work presented? If so, what are they? What is missing? What did you learn?\
(3) Depth: how deep did he/she go into the subject? Anything important missing? Comments? Suggestions for improvement?\
I expect at least 2 sentences for each of the above.\'a0\
\pard\pardeftab720

\f1\fs24 \cf0 \cb1 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
Comments about Gregory Bopp\'92s Report:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl320
\ls1\ilvl0
\f2 \cf0 \kerning1\expnd0\expndtw0 {\listtext	1.	}The main problem seems to be very clearly described. The conclusions are pretty clear to me except one thing. I could not understand exactly what was the hurdle to work on log scale while implementing the Bayesian elastic net estimator that incorporates model uncertainty.\
{\listtext	2.	}The work seems to be correct and legitimate. The conclusions were also clear except the point I mentioned above.\
		The LARS algorithm seems pretty intuitive unlike MH algorithm. This makes me wonder how many algorithms based on such simple ideas are waiting to be discovered. Augmenting the posterior with some tempering parameter and doing inference on that seems to be a very popular methodology like it was done here with tau. I did not know the theory behind how penalized likelihood approaches could be casted in the Bayesian formalism and this is described succinctly in this report. I am curious if likelihood free methods can be helpful in reducing the computational cost of the MH algorithm in this case. It would be interesting to try this and if this works out, we can do posterior inference on the parameters unlike the penalized likelihood approach.\
   3.	In my opinion, Greg went to reasonable depth into the subject and I did not find anything important missing. One thing I am curious is what effect does the choice of hyper parameters has on inference. Since he is taking these from the paper, it might be a good idea to understand how they choose it.\expnd0\expndtw0\kerning0
\uc0\u8232 \
}