
%! program = pdflatex

\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{float} % to keep the figures in place
\usepackage{placeins}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
%\usepackage{undertilde}
\usepackage{enumitem}
\newcommand{\cred}{ \color{red}}
\newcommand{\cgreen}{\color{green}}
\newcommand{\cblue}{\color{blue}}
\newcommand{\cmag}{\color{magenta}}
\newcommand{\bn}{\begin{enumerate}}
\newcommand{\en}{\end{enumerate}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{eqnarray}}
\newcommand{\ee}{\end{eqnarray}}
\newcommand{\by}{\begin{eqnarray*}}
\newcommand{\ey}{\end{eqnarray*}}
\renewcommand{\labelenumi}{(\alph{enumi}) }

\usepackage[margin=2.2cm, includehead]{geometry}% see geometry.pdf on how to lay out the page. There's lots.
\geometry{letterpaper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
%\bibpunct{(}{)}{;}{a}{,}{,}
%\setlength{\textwidth}{16cm}
%\setlength{\textheight}{21cm}
\def\nonumber{\global\@eqnswfalse}
\newcounter{parnum}
\newcommand{\N}{%
  \noindent\refstepcounter{parnum}%
   \makebox[\parindent][l]{\textbf{[\arabic{parnum}]}}\quad  }
% Use a generous paragraph indent so numbers can be fit inside the
% indentation space.
\setlength{\parindent}{1.5em}

% See the ``Article customise'' template for come common customisations

\date{}
%\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}
\SweaveOpts{concordance=TRUE}
%\large
%\maketitle
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{exam}[thm]{Example}
\newtheorem{qstn}[thm]{Question}

%%%
\newpage
\begin{center}
{\bf Homework 3 - STAT 540}\\
Amal Agarwal
\end{center}
%==========================
\section*{Answer 1}
\begin{enumerate}[label=(\alph*)]
\item
\begin{enumerate}[label=(\roman*)]
\item Psudocode for Nelder-Mead simplex algorithm:
\begin{itemize}
\item Initialization: Since the maximization of the log-likelihood function is over a three-dimensional space ($\mu, \xi, \sigma$), we minimize the negative log-likelihood function and start with an initial simplex S containing 4 vertices ($x_0, x_1, x_2, x_4$) such that $x_i \in \mathbb{R}^3$.
\item Iterations: Repeat the following steps till termination condition is satisfied.
\begin{itemize}
\item Ordering: In the current working simplex S, determine the indices $h,s,l$ of the worst, second worst and the best vertex, such that
\[f_h=\max_{j}f_j, f_s=\max_{j\neq h}f_j, f_l=\min_{j\neq h}f_j\]
\item Calculate the centroid c of the best side which is the one opposite to the worst vertex as $c:=\dfrac{1}{n}\sum_{j\neq h}x_j$
\item Transformation: Compute the new working simplex from the current one. First, replace only the worst vertex $x_h$ with a better point by using reflection, expansion or contraction with respect to the best side. All test points lie on the line defined by $x_h$ and $c$ , and at most two of them are computed in one iteration. If this succeeds, the accepted point becomes the new vertex of the working simplex. If this fails, shrink the simplex towards the best vertex $x_l$ . In this case, $n$ new vertices are computed. Simplex transformations in the Nelder-Mead method are controlled by four parameters satisfying the following constraints: $\alpha>0,0<\beta<1,\gamma>1,\gamma>\alpha,0<\delta<1$.

\begin{itemize}
\item Reflect: Compute the reflection point $x_r:=c+\alpha(c-x_h)$ and $f_r:=f(x_r)$. If $f_l\leq f_r<f_s$ , accept $x_r$ and terminate the iteration.
\item Expand: If $f_r<f_l$ , compute the expansion point $x_e:=c+\gamma(x_râˆ’c)$ and $f_e:=f(x_e)$ . If $f_e<f_r$ , accept $x_e$ and terminate the iteration. Otherwise (if $f_e \geq f_r$), accept $x_r$ and terminate the iteration.
\item Contract: If $f_r\geq f_s$ , compute the contraction point $x_c$ by using the better of the two points $x_h$ and $x_r$.
\begin{itemize}
\item Outside: If $f_s\leq f_r<f_h$ , compute $x_c:=c+\beta(x_r-c)$ and $f_c:=f(x_c)$. If $f_c\leq f_r$, accept $x_c$ and terminate the iteration.
Otherwise, perform a shrink transformation.
\item Inside: If $f_r\geq f_h$ , compute $x_c:=c+\beta(x_h-c)$ and $f_c:=f(x_c)$ . If $f_c<f_h$ , accept $x_c$ and terminate the iteration.
\end{itemize}
Otherwise, perform a shrink transformation.
\item Shrink: Compute $n$ new vertices $x_j:=x_l+\delta(x_j-x_l)$ and $f_j:=f(x_j)$, for $j=0,...,n$, with $j\neq l$ .
\end{itemize}
\end{itemize}
\item Termination: When the working simplex S is sufficiently small in some sense i.e. some or all vertices $x_j$ are close enough, terminate the algorithm.
\end{itemize}
(Reference: Scholarpedia)
\item Psudocode for Newton- Raphson algorithm:
\begin{itemize}
\item Start with an initial value $x^{(0)}=(\mu^{(0)},\xi^{(0)},\sigma^{(0)})$
\item Run loop until the termination condition is satisfied.
\begin{itemize}
\item $x^{(t+1)}=x^{(t)}-\nabla^2 l(x^{(t)}) \times \nabla l(x^{(t)})$ where $\nabla l(x^{(t)})$ is the gradient of log likelihood function evaluated at the the iterate $x^{(t)}=(\mu^{(t)},\xi^{(t)},\sigma^{(t)})$ and $\nabla^2 l(x^{(t)})$ is the hessian of log likelihood function evaluated at the the iterate $x^{(t)}=(\mu^{(t)},\xi^{(t)},\sigma^{(t)})$.
\item Check if $|x^{(t+1)}-x^{(t)}|<tol$ to terminate the loop. Here tol is some pre-specified tolerance.
\end{itemize}
\end{itemize}
 

\item Choosing the initial values: For both the above algorithms along with BFGS, the built-in function optim in R was used to calculate the MLE of ($\mu, \zeta, \sigma$) for the given data. In optim the initial values were passed as the maximizers of the log likelihood function over a grid of $1000$ points in the 3-dimensional parameter space. The end-points of the grid were randomly chosen as $(1,5)\times(0.1,5)\times(0.05,5.5)$.
\end{enumerate}

\item The following table gives the required information:
<<echo=F,results=hide>>=
#####################################################################
## Q1 (a)
## libraries
{library(ggplot2)
library(pracma)
library(reshape)
library(fExtremes)
library(plyr)
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}}

## Reading the data
data<-read.table("http://sites.stat.psu.edu/~mharan/540/hwdir/AtlanticGEV.dat",header = FALSE)
head(data)

## defining the negative of log likelihood for calculating MLE from given data
log.gev.dens<-function(x,mu,zeta,sigma){
  t4<--log(sigma)
  t1<-(1+zeta*((x-mu)/sigma))
  t2<--(t1^(-1/zeta))
  t3<-(-1-(1/zeta))*log(t1)
  value<-t2+t3+t4
  invisible(value)
}
log.lik<-function(param){
  mu<-param[1]
  zeta<-param[2]
  sigma<-param[3]
  if((sigma>0&&(zeta>0&&(prod(as.matrix(data)>(mu-(sigma/zeta)))==1))||(zeta<0&&(prod(as.matrix(data)<(mu-(sigma/zeta)))==1))||(zeta==0))){
    t4<--log(sigma)
    value<-sum(log.gev.dens(data[,1],mu = mu,zeta = zeta, sigma = sigma))
  }
  else {value<--(10^5)}
  invisible(-value)
}

## estimating initial parameter values by evaluating the log likelihood over a grid
## and choosing the maximum
mu.grid<-seq(1,2,length.out = 10)
zeta.grid<-seq(0.1,0.5,length.out = 10)
sigma.grid<-seq(0.05,0.5,length.out = 10)
par.grid<-expand.grid(mu.grid,zeta.grid,sigma.grid)
head(par.grid)
initial.index<-which.min(apply(X = par.grid,MARGIN = 1,FUN = log.lik))
initial.value<-par.grid[initial.index,]

## optimization using BFGS over above intial value
BFGS<-optim(par = initial.value, fn = log.lik, method = "BFGS",hessian = TRUE)
Info.inv<-solve(BFGS$hessian)

## extracting the estimates of mu, zeta and sigma for BFGS
mu.hat.BFGS<-as.numeric(BFGS$par[1])
mu.hat.BFGS
zeta.hat.BFGS<-as.numeric(BFGS$par[2])
zeta.hat.BFGS
sigma.hat.BFGS<-as.numeric(BFGS$par[3])
sigma.hat.BFGS

## caclulating the standard errors of mu, zeta and sigma for BFGS
mu.se.BFGS<-sqrt(Info.inv[1,1])
mu.se.BFGS
zeta.se.BFGS<-sqrt(Info.inv[2,2])
zeta.se.BFGS
sigma.se.BFGS<-sqrt(Info.inv[3,3])
sigma.se.BFGS

## caclulating the 95% confidence bounds of mu, zeta and sigma for BFGS
mu.lowc.BFGS<-mu.hat.BFGS-1.96*mu.se.BFGS
mu.upc.BFGS<-mu.hat.BFGS+1.96*mu.se.BFGS
zeta.lowc.BFGS<-zeta.hat.BFGS-1.96*zeta.se.BFGS
zeta.upc.BFGS<-zeta.hat.BFGS+1.96*zeta.se.BFGS
sigma.lowc.BFGS<-sigma.hat.BFGS-1.96*sigma.se.BFGS
sigma.upc.BFGS<-sigma.hat.BFGS+1.96*sigma.se.BFGS
  

## optimization using Nelder-Mead over above intial value
NM<-optim(par=c(1,3,1), fn = log.lik, method = "Nelder-Mead", hessian = TRUE)
Info.inv<-solve(NM$hessian)

## extracting the estimates of mu, zeta and sigma for Nelder-Mead
mu.hat.NM<-as.numeric(NM$par[1])
mu.hat.NM
zeta.hat.NM<-as.numeric(NM$par[2])
zeta.hat.NM
sigma.hat.NM<-as.numeric(NM$par[3])
sigma.hat.NM

## caclulating the standard errors of mu, zeta and sigma for Nelder-Mead
mu.se.NM<-sqrt(Info.inv[1,1])
mu.se.NM
zeta.se.NM<-sqrt(Info.inv[2,2])
zeta.se.NM
sigma.se.NM<-sqrt(Info.inv[3,3])
sigma.se.NM

## caclulating the 95% confidence bounds of mu, zeta and sigma for Nelder-Mead
mu.lowc.NM<-mu.hat.NM-1.96*mu.se.NM
mu.upc.NM<-mu.hat.NM+1.96*mu.se.NM
zeta.lowc.NM<-zeta.hat.NM-1.96*zeta.se.NM
zeta.upc.NM<-zeta.hat.NM+1.96*zeta.se.NM
sigma.lowc.NM<-sigma.hat.NM-1.96*sigma.se.NM
sigma.upc.NM<-sigma.hat.NM+1.96*sigma.se.NM

## optimization using Newton-Raphson over intial value obtained from Nelder Mead
## defining log likelihood for newton raphson
log.lik.n<-function(param){
  mu<-param[1]
  zeta<-param[2]
  sigma<-param[3]
  t4<--log(sigma)
  value<-sum(apply(X = as.matrix(data), MARGIN = 1, FUN = function(y){
    t1<-(1+zeta*((y-mu)/sigma))
    t2<--(t1^(-1/zeta))
    t3<-(-1-(1/zeta))*log(t1)
    t5<-t2+t3+t4
    invisible(t5)
    }))
  invisible(value)
}

## number of iterations for newton raphson
n<-100
## initializing the parameter estimates matrix 
param.NR<-matrix(NA_real_,3,n)
#param.NR[,1]<-c(jitter(NM$par,amount = 0))
param.NR[,1]<-c(1.64,0.18,0.09)

## initializing list for hessian matrix at each iterate
H<-list()
for (i in 1:(n-1)){
  H[[i]]<-hessian(log.lik.n,param.NR[,i],h=10^(-7))
  param.NR[,i+1]<-param.NR[,i]-(solve(H[[i]]))%*%grad(log.lik.n,param.NR[,i],heps=10^(-7))
  print(param.NR[,i+1])
}

Info.inv<--solve(hessian(log.lik.n,param.NR[,n],h=10^(-7)))

## extracting the estimates of mu, zeta and sigma for Newton Raphson
mu.hat.NR<-param.NR[1,n]
mu.hat.NR
zeta.hat.NR<-param.NR[2,n]
zeta.hat.NR
sigma.hat.NR<-param.NR[3,n]
sigma.hat.NR

## caclulating the standard errors of mu, zeta and sigma for Newton Raphson
mu.se.NR<-sqrt(Info.inv[1,1])
mu.se.NR
zeta.se.NR<-sqrt(Info.inv[2,2])
zeta.se.NR
sigma.se.NR<-sqrt(Info.inv[3,3])
sigma.se.NR

## caclulating the 95% confidence bounds of mu, zeta and sigma for Newton Raphson
mu.lowc.NR<-mu.hat.NR-1.96*mu.se.NR
mu.upc.NR<-mu.hat.NR+1.96*mu.se.NR
zeta.lowc.NR<-zeta.hat.NR-1.96*zeta.se.NR
zeta.upc.NR<-zeta.hat.NR+1.96*zeta.se.NR
sigma.lowc.NR<-sigma.hat.NR-1.96*sigma.se.NR
sigma.upc.NR<-sigma.hat.NR+1.96*sigma.se.NR

#########################################
## Q1 (c)
## plotting the histogram of samples and comparing with true GEV
x.vec.BFGS<-seq(mu.hat.BFGS-(sigma.hat.BFGS/zeta.hat.BFGS)+0.001,3,length.out = nrow(data))
y.vec.BFGS<-dgev(x.vec.BFGS, xi = zeta.hat.BFGS, mu = mu.hat.BFGS, beta = sigma.hat.BFGS)

x.vec.NM<-seq(mu.hat.NM-(sigma.hat.NM/zeta.hat.NM)+0.001,3,length.out = nrow(data))
y.vec.NM<-dgev(x.vec.NM, xi = zeta.hat.NM, mu = mu.hat.NM, beta = sigma.hat.NM)

x.vec.NR<-seq(mu.hat.NR-(sigma.hat.NR/zeta.hat.NR)+0.001,3,length.out = nrow(data))
y.vec.NR<-dgev(x.vec.NR, xi = zeta.hat.NR, mu = mu.hat.NR, beta = sigma.hat.NR)

df1<-data.frame(data,"BFGS"=x.vec.BFGS,"NM"=x.vec.NM,"NR"=x.vec.NR)
df2<-data.frame(data,"BFGS"=y.vec.BFGS,"NM"=y.vec.NM,"NR"=y.vec.NR)
df3<-melt(df1, id=c("V1"))
names(df3)<-c("data","method","x.vec")
df4<-data.frame(cbind(df3, "y.vec"=melt(df2, id=c("V1"))$value))
head(df4)

p<-ggplot(data = df4)
@
\begin{table}[H]
\centering
\caption{ML estimates and standard errors for the three algorithms}
\label{my-label}
\begin{tabular}{|c|c|c|}
\hline
               & ML estimates for ($\mu, \xi, \sigma$)& Standard errors for ($\mu, \xi, \sigma$)\\ \hline
BFGS           &(\Sexpr{round(mu.hat.BFGS,digits=2)},\Sexpr{round(zeta.hat.BFGS,digits=2)},\Sexpr{round(sigma.hat.BFGS,digits=3)})&(\Sexpr{round(mu.se.BFGS,digits=4)},\Sexpr{round(zeta.se.BFGS,digits=4)},\Sexpr{round(sigma.se.BFGS,digits=4)})\\ \hline
Nelder-Mead    &(\Sexpr{round(mu.hat.NM,digits=2)},\Sexpr{round(zeta.hat.NM,digits=2)},\Sexpr{round(sigma.hat.NM,digits=3)})&(\Sexpr{round(mu.se.NM,digits=4)},\Sexpr{round(zeta.se.NM,digits=4)},\Sexpr{round(sigma.se.NM,digits=4)})\\ \hline
Newton Raphson &(\Sexpr{round(mu.hat.NR,digits=2)},\Sexpr{round(zeta.hat.NR,digits=2)},\Sexpr{round(sigma.hat.NR,digits=3)})&(\Sexpr{round(mu.se.NR,digits=4)},\Sexpr{round(zeta.se.NR,digits=4)},\Sexpr{round(sigma.se.NR,digits=4)})\\ \hline
\end{tabular}
\end{table}

Standard error estimates of ($\mu, \xi, \sigma$) were found by inverting the observed information matrix and taking the square root of the diagnol entries which correspond to ($\mu, \xi, \sigma$) respectively. This is using the asymptotic theory for MLE's which gives the following convergence in law.
\[\sqrt{n}(\hat{\theta_n}-\theta)\longrightarrow N(0, I^{-1}(\theta))\]
Now we know that the observed information is the negative of the Hessian matrix. For the first two algorithms BFGS and Nelder-Mead, the negative Hessian matrix can be extracted from the optim function. For the Newton-Raphson algorithm, the Hessian matrix was computed for the last iterate using "hessian" function from the package "pracma". Note that in all the above cases the observed information is for the whole dataset (not just one data point) and so division by $n$ is not required. 

\item The following figure gives the fitted GEV density for all the three optimization algorithms over histogram of the data. 

\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
p+geom_histogram(mapping = aes(x=data,y=..density..),fill="steelblue",binwidth=0.04)+labs(x="x",y="Density")+geom_line(mapping = aes(x = x.vec,y = y.vec,colour=method))
@
\caption{Plot of fitted density for three algorithms over the histogram of the data}
\end{centering}
\end{figure}
Clearly the lines seem to coincide which shows that the parameter estimates are really close.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Answer 2}
\begin{enumerate}[label=(\alph*)]
\item 
\begin{enumerate}[label=(\roman*)]
\item
Pseudocode for the first Metropolis Hastings algorithm (Variable one at a time) for the given target density:
\begin{itemize}
\item Start off with $x^{0}=(\mu^{(0)},\xi^{(0)},\sigma^{(0)})$ chosen suitably after a series of pilot runs. Choose the length of the Markov chain as $m$.
\item Run loop from $t=0:(m-1)$
\begin{itemize}
\item Assign the current state as $x^{(t)}=(\mu^{(t)},\xi^{(t)},\sigma^{(t)})$.
\item Generate a candidate $\mu^{*}$ drawn from univariate normal distribution with mean as $\mu^{(t)}$ and variance $\tau_{\mu}^2$ being the tuning parameter.
\item Accept $\mu^{*}$ as next state $\mu^{(t+1)}$ with the following acceptance probability $\alpha_\mu$, else assign the current state as next state.
\[\alpha_\mu(\mu^{*},\mu^{(t)})=\text{min}\left(1,\dfrac{h_{\mu}(\mu^{*},\xi^{(t)},\sigma^{(t)})q_\mu(\mu^{*},\mu^{(t)})}{h_\mu(\mu^{(t)},\xi^{(t)},\sigma^{(t)})q_\mu(\mu^{(t)},\mu^{*})}\right)=\text{min}\left(1,\dfrac{h_\mu(\mu^{*},\xi^{(t)},\sigma^{(t)})}{h_\mu(\mu^{(t)},\xi^{(t)},\sigma^{(t)})}\right)\]
where $h_\mu$ is the conditional target kernel for $\mu$ which the product of the likelihood function and the uniform prior (contains no term for the given prior) and $q_\mu$ is univariate normal proposal density which cancels out being symmetric.

\item Generate a candidate $\xi^{*}$ drawn from univariate normal distribution with mean as $\xi^{(t)}$ and variance $\tau_{\xi}^2$ being the tuning parameter.
\item Accept $\xi^{*}$ as next state $\xi^{(t+1)}$ with the following acceptance probability $\alpha_\xi$, else assign the current state as next state.
\[\alpha_\xi(\xi^{*},\xi^{(t)})=\text{min}\left(1,\dfrac{h_\xi(\mu^{(t+1)},\xi^{*},\sigma^{(t)})q_\xi(\xi^{*},\xi^{(t)})}{h_\xi(\mu^{(t+1)},\xi^{(t)},\sigma^{(t)})q_\xi(\xi^{(t)},\xi^{*})}\right)=\text{min}\left(1,\dfrac{h_\xi(\mu^{(t+1)},\xi^{*},\sigma^{(t)})}{h_\xi(\mu^{(t+1)},\xi^{(t)},\sigma^{(t)})}\right)\]
where $h_\xi$ is the conditional target kernel for $\xi$ which is the product of the likelihood function and the uniform prior (contains no term for the given prior) and $q_\xi$ is univariate normal proposal density which cancels out being symmetric.

\item Generate a candidate $\sigma^{*}$ drawn from univariate truncated normal distribution with bounds as $(0,\infty)$. Again take the mean of this proposal as $\sigma^{(t)}$ and variance as $\tau_{\sigma}^2$ being the tuning parameter.
\item Accept $\sigma^{*}$ as next state $\sigma^{(t+1)}$ with the following acceptance probability $\alpha_\sigma$, else assign the current state as next state.
\[\alpha_\sigma(\sigma^{*},\sigma^{(t)})=\text{min}\left(1,\dfrac{h_\sigma(\mu^{(t+1)},\xi^{(t+1)},\sigma^{*})q_\sigma(\sigma^{*},\sigma^{(t)})}{h_\sigma(\mu^{(t+1)},\xi^{(t+1)},\sigma^{(t)})q_\sigma(\sigma^{t},\sigma^{*})}\right)\]
where $h_\sigma$ is the conditional target kernel for $\sigma$ which is the product of the likelihood function and the uniform prior and $q_\sigma$ is univariate truncated normal proposal density.

\item Assign the next state as $x^{(t+1)}=(\mu^{(t+1)},\xi^{(t+1)},\sigma^{(t+1)})$.
\end{itemize}
\end{itemize}
\item Pseudocode for the second Metropolis Hastings algorithm (Variable one at a time) for the given target density is same as the first one above with just the change in proposal density $q_\sigma$ for $\sigma$ as gamma density instead of univariate truncated normal. The mean of $q_\sigma$ is chosen as the current state and the variance is kept as the tuning parameter.
\item As before, starting values for both the algorithms were chosen as the maximizers of the log likelihood function over a grid of $1000$ points in the 3-dimensional parameter space. The end-points of the grid were randomly chosen as $(1,5)\times(0.1,5)\times(0.05,5.5)$.
\end{enumerate}
\item For both the algorithms, the appropriate chain length was chosen as $2500$ since the all the diagnostics shown below suggest that the convergence takes place till after this point. Besides, the relative error i.e. the ratio of MCMC standard error to the estimate drops below $10^{-2}$ for $n>2500$ which is reasonably good. 
\clearpage
\begin{enumerate}[label=(\roman*)]
\item Diagnostics for algorithm 1:
<<echo=F, results=hide>>=
## First M-H algo:
## defining the function Expectation1 for Variable at a time Metropolis Hastings
## algortithm and to calculate Monte Carlo estimates and standard errors
## for x
Expectation1<-function(n,start,var.tune){
  ## libraries
  {library(ggplot2)
    library(tmvtnorm)
    library(fExtremes)
    library(plyr)
    multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
      library(grid)
      
      # Make a list from the ... arguments and plotlist
      plots <- c(list(...), plotlist)
      
      numPlots = length(plots)
      
      # If layout is NULL, then use 'cols' to determine layout
      if (is.null(layout)) {
        # Make the panel
        # ncol: Number of columns of plots
        # nrow: Number of rows needed, calculated from # of cols
        layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                         ncol = cols, nrow = ceiling(numPlots/cols))
      }
      
      if (numPlots==1) {
        print(plots[[1]])
        
      } else {
        # Set up the page
        grid.newpage()
        pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
        
        # Make each plot, in the correct location
        for (i in 1:numPlots) {
          # Get the i,j matrix positions of the regions that contain this subplot
          matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
          
          print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                          layout.pos.col = matchidx$col))
        }
      }
    }}
  
  ## Reading the data
  data<-read.table("http://sites.stat.psu.edu/~mharan/540/hwdir/AtlanticGEV.dat",header = FALSE)
  head(data)
  
  ## sourcing batchmeans function used to calculate MCMC standard
  ## errors later
  source("http://www.stat.psu.edu/~mharan/batchmeans.R")
  
  log.gev.dens<-function(x,mu,zeta,sigma){
    t4<--log(sigma)
    t1<-(1+zeta*((x-mu)/sigma))
    t2<--(t1^(-1/zeta))
    t3<-(-1-(1/zeta))*log(t1)
    value<-t2+t3+t4
    invisible(value)
  }
  
  ## defining log likelihood function as a part of target
  log.lik<-function(param){
    mu<-param[1]
    zeta<-param[2]
    sigma<-param[3]
    if((sigma>0&&(zeta>0&&(prod(as.matrix(data)>(mu-(sigma/zeta)))==1))||(zeta<0&&(prod(as.matrix(data)<(mu-(sigma/zeta)))==1))||(zeta==0))){
      t4<--log(sigma)
      value<-sum(log.gev.dens(data,mu = mu,zeta = zeta, sigma = sigma))
    }
    else {value<-NA}
    invisible(value)
  }
  
  ## Variable at a time MCMC
  ## Metropolis Hastings function for mu with inputs as the variance of proposal
  ## (tuning parameter) and current state of the MC
  MH.mu<-function(current.state,var.tune){
    ## sampling x.star from normal proposal with mean as current state
    ## of x and specified tuning parameter
    x.star<-rnorm(n=1, mean=current.state[1], sd = sqrt(var.tune))
    if (!is.na(log.lik(c(x.star,current.state[2],current.state[3])))){
      num<-log.lik(c(x.star,current.state[2],current.state[3]))
      denom<-log.lik(current.state)
      ## defining the acceptance probability for sampled x.star on log
      ## scale
      accept.probab<-num-denom
      ## sampling u from uniform(0,1) to check for acceptance
      u<-runif(1, min=0, max=1)
      ## initializing the indicator flag=0 to check if the sampled x.star
      ## will be accepted
      flag<-0
      ## if-else to define the next state of the chain based on acceptance
      ## probability
      if(log(u)<=accept.probab){
        flag<-1
        next.state<-x.star
      }
      else {next.state<-current.state[1]}
    }
    else {
      next.state<-current.state[1]
      flag<-0
    }
    ## returning the next state and indicator if the sampled value was
    ## accepted
    return(c(next.state,flag))
  }
  ## Metropolis Hastings function for zeta with inputs as the variance of proposal
  ## (tuning parameter) and current state of the MC
  MH.zeta<-function(current.state,var.tune){
    ## sampling x.star from normal proposal with mean as current state
    ## of x and specified tuning parameter
    x.star<-rnorm(n=1, mean=current.state[2], sd = sqrt(var.tune))
    if(!is.na(log.lik(c(current.state[1],x.star,current.state[3])))){
      num<-log.lik(c(current.state[1],x.star,current.state[3]))
      denom<-log.lik(current.state)
      ## defining the acceptance probability for sampled x.star on log
      ## scale
      accept.probab<-num-denom
      ## sampling u from uniform(0,1) to check for acceptance
      u<-runif(1, min=0, max=1)
      ## initializing the indicator flag=0 to check if the sampled x.star
      ## will be accepted
      flag<-0
      ## if-else to define the next state of the chain based on acceptance
      ## probability
      if(log(u)<=accept.probab){
        flag<-1
        next.state<-x.star
      }
      else {next.state<-current.state[2]}
    }
    else {
      next.state<-current.state[2]
      flag<-0
    }
    ## returning the next state and indicator if the sampled value was
    ## accepted
    return(c(next.state,flag))
  }
  ## Metropolis Hastings function for sigma with inputs as the variance of proposal
  ## (tuning parameter) and current state of the MC
  MH.sigma<-function(current.state,var.tune){
    ## sampling x.star from normal proposal with mean as current state
    ## of x and specified tuning parameter
    x.star<-rtmvnorm(n=1, mean=current.state[3], sigma = var.tune, lower = 0)
    
    if(!is.na(log.lik(c(current.state[1],current.state[2],x.star)))){
      num<-log.lik(c(current.state[1],current.state[2],x.star))-log(x.star)+dtmvnorm(current.state[3], mean=as.vector(x.star), sigma = var.tune, lower = 0,log = TRUE)
      denom<-log.lik(current.state)-log(current.state[3])+dtmvnorm(x.star, mean=as.vector(current.state[3]), sigma = var.tune, lower = 0, log = TRUE)
      ## defining the acceptance probability for sampled x.star on log
      ## scale
      accept.probab<-num-denom
      ## sampling u from uniform(0,1) to check for acceptance
      u<-runif(1, min=0, max=1)
      ## initializing the indicator flag=0 to check if the sampled x.star
      ## will be accepted
      flag<-0
      ## if-else to define the next state of the chain based on acceptance
      ## probability
      if(log(u)<=accept.probab){
        flag<-1
        next.state<-x.star
      }
      else {next.state<-current.state[3]}
    }
    else {
      next.state<-current.state[3]
      flag<-0
    }
    ## returning the next state and indicator if the sampled value was
    ## accepted
    return(c(next.state,flag))
  }
  
  ## Initializing the Markov chain for beta_1
  x<-matrix(NA_real_,3,n)
  ## Defining the initial value for the chain
  x[,1]<-start
  ## Initializing the accept count for mu, zeta, sigma used to calculate acceptance
  ## rate of x
  accept.mu<-0
  accept.zeta<-0
  accept.sigma<-0
  ## loop for RWM updates
  sys.time<-system.time(for(i in 1:(n-1)){
    curr.state<-x[,i]
    
    temp<-MH.mu(curr.state,var.tune[1])
    curr.state[1]<-temp[1]
    accept.mu<-accept.mu+temp[2]
    
    temp<-MH.zeta(curr.state,var.tune[2])
    curr.state[2]<-temp[1]
    accept.zeta<-accept.zeta+temp[2]
    
    temp<-MH.sigma(curr.state,var.tune[3])
    curr.state[3]<-temp[1]
    accept.sigma<-accept.sigma+temp[2]
    
    x[,i+1]<-curr.state
  })
  ## samples obtained from the running the chain for given n
  samples<-data.frame("iterations"=1:n,"X1"=t(x)[,1],"X2"=t(x)[,2],"X3"=t(x)[,3])
  ## calculating the acceptance rates for mu, zeta and sigma
  acceptance.rate<-c((accept.mu/n),(accept.zeta/n),(accept.sigma/n))
  
  ## defining the sequence for number of iterations
  m<-seq(100,n,by=100)
  ## length of the sequence m
  ml<-length(m)
  
  ## Initializing a matrix MCMC.means for storing MCMC estimates
  MCMC.means<-matrix(NA_real_,3,ml)
  ## Initializing a matrix MCMC.se for storing MCMC standard errors for the 3 components
  ## separately
  MCMC.se<-matrix(NA_real_,3,ml)
  ## loop for storing Monte Carlo estimates and MCMC standard errors in R
  for (j in 1:ml){
    MCMC.means[,j]<-rowSums(x[,1:m[j]])/m[j]
    ## Using the batchmeans function to calculate MCMC standard errors
    MCMC.se[1,j]<-bm(x[1,1:m[j]])$se
    MCMC.se[2,j]<-bm(x[2,1:m[j]])$se
    MCMC.se[3,j]<-bm(x[3,1:m[j]])$se
  }
  ESS.10K<-c(ess(samples[(1:10000),2]),ess(samples[(1:10000),3]),ess(samples[(1:10000),4]))
  ESS<-c(ess(samples[,2]),ess(samples[,3]),ess(samples[,4]))
  ESS.rate<-ESS/as.numeric(sys.time[3])
  return(list("samples"=samples,"iterations"=m, "means"=MCMC.means, "MCMC.se"=MCMC.se, "acceptance.rate"=acceptance.rate,"ESS.10K"=ESS.10K,"ESS.rate"=ESS.rate))
}
## defining the chain size
n<-10000
start<-matrix(NA_real_,3,3)
start[,1]<-c(1.65,0.172,0.098)
for (j in 2:3){
  start[,j]<-jitter(start[,1],amount = 0)
}
var.tune<-c(0.00000436,0.000420,(2.88*(10^(-6))))

M<-vector("list",ncol(start))

library(foreach)
library(doParallel)
cl <- makeCluster(3)
registerDoParallel(cl)
M1<-foreach (j = 1:3) %dopar% {
  Expectation1(n,start[,j],var.tune)
}
stopCluster(cl)
M<-M1

## calculating the sample quantiles for credible intervals
mu.lowc.1<-quantile(M[[1]][[1]][,2],0.025)
mu.upc.1<-quantile(M[[1]][[1]][,2],0.975)
zeta.lowc.1<-quantile(M[[1]][[1]][,3],0.025)
zeta.upc.1<-quantile(M[[1]][[1]][,3],0.975)
sigma.lowc.1<-quantile(M[[1]][[1]][,4],0.025)
sigma.upc.1<-quantile(M[[1]][[1]][,4],0.975)

mu.se.p1<-sqrt(var(M[[1]][[1]][,2]))
zeta.se.p1<-sqrt(var(M[[1]][[1]][,3]))
sigma.se.p1<-sqrt(var(M[[1]][[1]][,4]))

###########################################
## Q2 (b)
## plotting
library(ggplot2)
thema<-theme_bw(base_size = 20) +
  theme(axis.title.x = element_text(size = 8, colour = "black"), 
        axis.text.x  = element_text(angle = 0, size = 8, colour = "black"),
        axis.title.y = element_text(size = 8, colour = "black"), 
        axis.text.y  = element_text(angle = 0, size = 8, colour = "black"),
        legend.text  = element_text(size = 8, colour = "black"), 
        legend.title = element_text(size = 8, colour = "black"),
        panel.background = element_rect(fill = "white"),
        panel.grid.major = element_line(colour = "white", linetype = NULL),
        panel.grid.minor = element_line(colour = "white", linetype = NULL),
        text = element_text(size = 8, colour = "black"),
        title =  element_text(size = 8, face = "bold"))

## defining the data frames for plotting
f<-list()
g<-list()
for(j in 1:ncol(start)){
  f[[j]]<-data.frame(M[[j]][[2]],t(M[[j]][[3]]),t(M[[j]][[4]]),j)
  g[[j]]<-data.frame((M[[j]][[1]]),j)
}

f<-do.call(rbind,f)
names(f)<-c("iterations","mean.X1","mean.X2","mean.X3","MCMCse.X1","MCMCse.X2","MCMCse.X3","start.label")

g<-do.call(rbind,g)
names(g)<-c("iterations","samples.X1","samples.X2","samples.X3","start.label")

p<-ggplot(data=f)
q<-ggplot(data=g)
@
Plot of the ACF of samples is given as:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
acf(M[[1]][[1]][,-1],lag.max = 50,main="Plot of ACF of samples")
@
\caption{Plot of ACF of samples}
\end{centering}
\end{figure}
For all the 3 components, the above plot is reasonably good which indicates that the tuning parameter value of $(4.36\times 10^{-6}, 4.20\times 10^{-4}, 2.88\times 10^{-6})$ may not be the best but still it works reasonably good.

For 3 different starting values as (\Sexpr{round(start[1,1],digits=2)},\Sexpr{round(start[2,1],digits=2)},\Sexpr{round(start[3,1],digits=3)}), (\Sexpr{round(start[1,2],digits=2)},\Sexpr{round(start[2,2],digits=2)},\Sexpr{round(start[3,2],digits=3)}) and (\Sexpr{round(start[1,3],digits=2)},\Sexpr{round(start[2,3],digits=2)},\Sexpr{round(start[3,3],digits=3)}), labelled as $1$, $2$ and $3$, plots of estimates of expected values of $(\mu,\xi,\sigma)$ and Monte Carlo standard errors with sample size are given below:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## plotting mean vs. sample size for different starting values
p1<-p+ geom_line(mapping = aes(x=iterations,y=mean.X1,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))+labs(x="Number of samples",y=expression(paste("Estimate of E(", mu ,")")), colour="Label for starting values")+thema+
  geom_errorbar(mapping = aes(x=iterations,y=mean.X1,ymin=mean.X1-MCMCse.X1,ymax=mean.X1+MCMCse.X1,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))
p2<-p+ geom_line(mapping = aes(x=iterations,y=mean.X2,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))+labs(x="Number of samples",y=expression(paste("Estimate of E(", xi ,")")), colour="Label for starting values")+thema+
  geom_errorbar(mapping = aes(x=iterations,y=mean.X2,ymin=mean.X2-MCMCse.X2,ymax=mean.X2+MCMCse.X2,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))
p3<-p+ geom_line(mapping = aes(x=iterations,y=mean.X3,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))+labs(x="Number of samples",y=expression(paste("Estimate of E(", sigma ,")")), colour="Label for starting values")+thema+
  geom_errorbar(mapping = aes(x=iterations,y=mean.X3,ymin=mean.X3-MCMCse.X3,ymax=mean.X3+MCMCse.X3,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))
multiplot(p1, p2, p3,cols=1)
@
\caption{Plot of estimates vs. sample size with error bars for $(\mu,\xi,\sigma)$}
\end{centering}
\end{figure}

\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## plotting (MCMCse) vs. sample size for different starting values
p4<-p+ geom_line(mapping = aes(x=iterations,y=(MCMCse.X1),group=factor(start.label),colour=factor(start.label)))+labs(x="Number of samples",y=expression(paste("MCMCse for", mu)),colour="Label for starting values")+thema
p5<-p+ geom_line(mapping = aes(x=iterations,y=(MCMCse.X2),group=factor(start.label),colour=factor(start.label)))+labs(x="Number of samples",y=expression(paste("MCMCse for", xi)),colour="Label for starting values")+thema
p6<-p+ geom_line(mapping = aes(x=iterations,y=(MCMCse.X3),group=factor(start.label),colour=factor(start.label)))+labs(x="Number of samples",y=expression(paste("MCMCse for", sigma)),colour="Label for starting values")+thema
multiplot(p4, p5, p6,cols=1)
@
\caption{Plot of MCMC se vs. sample size for $(\mu,\xi,\sigma)$}
\end{centering}
\end{figure}
The above plots show that for all the 3 components and for different starting alues, the estimates converge to same value and MCMC standard errors converge to 0. This verifies the algorithm to some extent.
Plotting the estimated density after $n/2$ and after $n$:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## estimated marginal densities after N/2 and after N
q1<-q+ geom_density(mapping = aes(x=samples.X1),fill="steelblue",subset=.(start.label==1&iterations[1:(length(iterations)/2)]))+
  labs(x="",y=expression(paste("For", mu)),title="After N/2")+thema
q2<-q+geom_density(mapping = aes(x=samples.X1),fill="tomato",subset=.(start.label==1))+
  labs(x="",y=expression(paste("For", mu)),title="After N")+thema

q3<-q+ geom_density(mapping = aes(x=samples.X2),fill="steelblue",subset=.(start.label==1&iterations[1:(length(iterations)/2)]))+
  labs(x="",y=expression(paste("For", xi)),title="After N/2")+thema
q4<-q+geom_density(mapping = aes(x=samples.X2),fill="tomato",subset=.(start.label==1))+
  labs(x="",y=expression(paste("For", xi)),title="After N")+thema

q5<-q+ geom_density(mapping = aes(x=samples.X3),fill="steelblue",subset=.(start.label==1&iterations[1:(length(iterations)/2)]))+
  labs(x="",y=expression(paste("For", sigma)),title="After N/2")+thema
q6<-q+geom_density(mapping = aes(x=samples.X3),fill="tomato",subset=.(start.label==1))+
  labs(x="",y=expression(paste("For", sigma)),title="After N")+thema

multiplot(q1,q3,q5,q2,q4,q6, cols=2)
@
\caption{Plot of estimated density for $(\mu,\xi,\sigma)$ after N/2 and N}
\end{centering}
\end{figure}
\clearpage

Plotting the estimated density for different starting values for all the components:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## estimated density for different starting values
q7<-q+ geom_density(mapping = aes(x=samples.X1,group=factor(start.label),colour=factor(start.label)))+
  labs(x="",y=expression(paste("For", mu)),colour="Label of starting values")+thema
q8<-q+ geom_density(mapping = aes(x=samples.X2,group=factor(start.label),colour=factor(start.label)))+
  labs(x="",y=expression(paste("For", xi)),colour="Label of starting values")+thema
q9<-q+ geom_density(mapping = aes(x=samples.X3,group=factor(start.label),colour=factor(start.label)))+
  labs(x="",y=expression(paste("For", sigma)),colour="Label of starting values")+thema
multiplot(q7,q8,q9, cols=1)
@
\caption{Plot of marginal density for $(\mu,\xi,\sigma)$}
\end{centering}
\end{figure}
From the above plots, the estimated densities after $n/2$ and after $n$ look reasonably identical for all the 3 components. Also the estimated density for different starting values also overlap to a good extent. This further verifies the robustness and convergence of the algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Diagnostics for algorithm 2:
<<echo=F, results=hide>>=
## Second M-H algo:
## defining the function Expectation2 for Variable at a time Metropolis Hastings
## algortithm and to calculate Monte Carlo estimates and standard errors
## for x
Expectation2<-function(n,start,var.tune){
  ## libraries
  {library(ggplot2)
    library(tmvtnorm)
    library(fExtremes)
    library(plyr)
    multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
      library(grid)
      
      # Make a list from the ... arguments and plotlist
      plots <- c(list(...), plotlist)
      
      numPlots = length(plots)
      
      # If layout is NULL, then use 'cols' to determine layout
      if (is.null(layout)) {
        # Make the panel
        # ncol: Number of columns of plots
        # nrow: Number of rows needed, calculated from # of cols
        layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                         ncol = cols, nrow = ceiling(numPlots/cols))
      }
      
      if (numPlots==1) {
        print(plots[[1]])
        
      } else {
        # Set up the page
        grid.newpage()
        pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
        
        # Make each plot, in the correct location
        for (i in 1:numPlots) {
          # Get the i,j matrix positions of the regions that contain this subplot
          matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
          
          print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                          layout.pos.col = matchidx$col))
        }
      }
    }}
  
  ## Reading the data
  data<-read.table("http://sites.stat.psu.edu/~mharan/540/hwdir/AtlanticGEV.dat",header = FALSE)
  head(data)
  
  ## sourcing batchmeans function used to calculate MCMC standard
  ## errors later
  source("http://www.stat.psu.edu/~mharan/batchmeans.R")
  
  log.gev.dens<-function(x,mu,zeta,sigma){
    t4<--log(sigma)
    t1<-(1+zeta*((x-mu)/sigma))
    t2<--(t1^(-1/zeta))
    t3<-(-1-(1/zeta))*log(t1)
    value<-t2+t3+t4
    invisible(value)
  }
  
  ## defining log likelihood function as a part of target
  log.lik<-function(param){
    mu<-param[1]
    zeta<-param[2]
    sigma<-param[3]
    if((sigma>0&&(zeta>0&&(prod(as.matrix(data)>(mu-(sigma/zeta)))==1))||(zeta<0&&(prod(as.matrix(data)<(mu-(sigma/zeta)))==1))||(zeta==0))){
      t4<--log(sigma)
      value<-sum(log.gev.dens(data,mu = mu,zeta = zeta, sigma = sigma))
    }
    else {value<-NA}
    invisible(value)
  }
  
  ## Variable at a time MCMC
  ## Metropolis Hastings function for mu with inputs as the variance of proposal
  ## (tuning parameter) and current state of the MC
  MH.mu<-function(current.state,var.tune){
    ## sampling x.star from normal proposal with mean as current state
    ## of x and specified tuning parameter
    x.star<-rnorm(n=1, mean=current.state[1], sd = sqrt(var.tune))
    if (!is.na(log.lik(c(x.star,current.state[2],current.state[3])))){
      num<-log.lik(c(x.star,current.state[2],current.state[3]))
      denom<-log.lik(current.state)
      ## defining the acceptance probability for sampled x.star on log
      ## scale
      accept.probab<-num-denom
      ## sampling u from uniform(0,1) to check for acceptance
      u<-runif(1, min=0, max=1)
      ## initializing the indicator flag=0 to check if the sampled x.star
      ## will be accepted
      flag<-0
      ## if-else to define the next state of the chain based on acceptance
      ## probability
      if(log(u)<=accept.probab){
        flag<-1
        next.state<-x.star
      }
      else {next.state<-current.state[1]}
    }
    else {
      next.state<-current.state[1]
      flag<-0
    }
    ## returning the next state and indicator if the sampled value was
    ## accepted
    return(c(next.state,flag))
  }
  ## Metropolis Hastings function for zeta with inputs as the variance of proposal
  ## (tuning parameter) and current state of the MC
  MH.zeta<-function(current.state,var.tune){
    ## sampling x.star from normal proposal with mean as current state
    ## of x and specified tuning parameter
    x.star<-rnorm(n=1, mean=current.state[2], sd = sqrt(var.tune))
    if(!is.na(log.lik(c(current.state[1],x.star,current.state[3])))){
      num<-log.lik(c(current.state[1],x.star,current.state[3]))
      denom<-log.lik(current.state)
      ## defining the acceptance probability for sampled x.star on log
      ## scale
      accept.probab<-num-denom
      ## sampling u from uniform(0,1) to check for acceptance
      u<-runif(1, min=0, max=1)
      ## initializing the indicator flag=0 to check if the sampled x.star
      ## will be accepted
      flag<-0
      ## if-else to define the next state of the chain based on acceptance
      ## probability
      if(log(u)<=accept.probab){
        flag<-1
        next.state<-x.star
      }
      else {next.state<-current.state[2]}
    }
    else {
      next.state<-current.state[2]
      flag<-0
    }
    ## returning the next state and indicator if the sampled value was
    ## accepted
    return(c(next.state,flag))
  }
  ## Metropolis Hastings function for sigma with inputs as the variance of proposal
  ## (tuning parameter) and current state of the MC
  MH.sigma<-function(current.state,var.tune){
    ## sampling x.star from normal proposal with mean as current state
    ## of x and specified tuning parameter
    x.star<-rgamma(n=1, shape=(current.state[3]^2)/var.tune, rate = current.state[3]/var.tune)
    
    if(!is.na(log.lik(c(current.state[1],current.state[2],x.star)))){
      num<-log.lik(c(current.state[1],current.state[2],x.star))-log(x.star)+dgamma(current.state[3], shape = (x.star^2)/var.tune, rate = x.star/var.tune,log = TRUE)
      denom<-log.lik(current.state)-log(current.state[3])+dgamma(x.star, shape = (current.state[3]^2)/var.tune, rate = current.state[3]/var.tune, log = TRUE)
      ## defining the acceptance probability for sampled x.star on log
      ## scale
      accept.probab<-num-denom
      ## sampling u from uniform(0,1) to check for acceptance
      u<-runif(1, min=0, max=1)
      ## initializing the indicator flag=0 to check if the sampled x.star
      ## will be accepted
      flag<-0
      ## if-else to define the next state of the chain based on acceptance
      ## probability
      if(log(u)<=accept.probab){
        flag<-1
        next.state<-x.star
      }
      else {next.state<-current.state[3]}
    }
    else {
      next.state<-current.state[3]
      flag<-0
    }
    ## returning the next state and indicator if the sampled value was
    ## accepted
    return(c(next.state,flag))
  }
  
  ## Initializing the Markov chain for beta_1
  x<-matrix(NA_real_,3,n)
  ## Defining the initial value for the chain
  x[,1]<-start
  ## Initializing the accept count for mu, zeta, sigma used to calculate acceptance
  ## rate of x
  accept.mu<-0
  accept.zeta<-0
  accept.sigma<-0
  ## loop for RWM updates
  sys.time<-system.time(for(i in 1:(n-1)){
    curr.state<-x[,i]
    
    temp<-MH.mu(curr.state,var.tune[1])
    curr.state[1]<-temp[1]
    accept.mu<-accept.mu+temp[2]
    
    temp<-MH.zeta(curr.state,var.tune[2])
    curr.state[2]<-temp[1]
    accept.zeta<-accept.zeta+temp[2]
    
    temp<-MH.sigma(curr.state,var.tune[3])
    curr.state[3]<-temp[1]
    accept.sigma<-accept.sigma+temp[2]
    
    x[,i+1]<-curr.state
  })
  ## samples obtained from the running the chain for given n
  samples<-data.frame("iterations"=1:n,"X1"=t(x)[,1],"X2"=t(x)[,2],"X3"=t(x)[,3])
  ## calculating the acceptance rates for mu, zeta and sigma
  acceptance.rate<-c((accept.mu/n),(accept.zeta/n),(accept.sigma/n))
  
  ## defining the sequence for number of iterations
  m<-seq(100,n,by=100)
  ## length of the sequence m
  ml<-length(m)
  
  ## Initializing a matrix MCMC.means for storing MCMC estimates
  MCMC.means<-matrix(NA_real_,3,ml)
  ## Initializing a matrix MCMC.se for storing MCMC standard errors for the 3 components
  ## separately
  MCMC.se<-matrix(NA_real_,3,ml)
  ## loop for storing Monte Carlo estimates and MCMC standard errors in R
  for (j in 1:ml){
    MCMC.means[,j]<-rowSums(x[,1:m[j]])/m[j]
    ## Using the batchmeans function to calculate MCMC standard errors
    MCMC.se[1,j]<-bm(x[1,1:m[j]])$se
    MCMC.se[2,j]<-bm(x[2,1:m[j]])$se
    MCMC.se[3,j]<-bm(x[3,1:m[j]])$se
  }
  ESS.10K<-c(ess(samples[(1:10000),2]),ess(samples[(1:10000),3]),ess(samples[(1:10000),4]))
  ESS<-c(ess(samples[,2]),ess(samples[,3]),ess(samples[,4]))
  ESS.rate<-ESS/as.numeric(sys.time[3])
  return(list("samples"=samples,"iterations"=m, "means"=MCMC.means, "MCMC.se"=MCMC.se, "acceptance.rate"=acceptance.rate,"ESS.10K"=ESS.10K,"ESS.rate"=ESS.rate))
}
## defining the chain size
n<-10000
start<-matrix(NA_real_,3,3)
start[,1]<-c(1.65,0.172,0.098)
for (j in 2:3){
  start[,j]<-jitter(start[,1],amount = 0)
}
var.tune<-c(0.00000436,0.000420,(2.88*(10^(-6))))

M<-vector("list",ncol(start))

library(foreach)
library(doParallel)
cl <- makeCluster(3)
registerDoParallel(cl)
M2<-foreach (j = 1:3) %dopar% {
  Expectation2(n,start[,j],var.tune)
}
stopCluster(cl)
M<-M2

## calculating the sample quantiles for credible intervals
mu.lowc.2<-quantile(M[[1]][[1]][,2],0.025)
mu.upc.2<-quantile(M[[1]][[1]][,2],0.975)
zeta.lowc.2<-quantile(M[[1]][[1]][,3],0.025)
zeta.upc.2<-quantile(M[[1]][[1]][,3],0.975)
sigma.lowc.2<-quantile(M[[1]][[1]][,4],0.025)
sigma.upc.2<-quantile(M[[1]][[1]][,4],0.975)

mu.se.p2<-sqrt(var(M[[1]][[1]][,2]))
zeta.se.p2<-sqrt(var(M[[1]][[1]][,3]))
sigma.se.p2<-sqrt(var(M[[1]][[1]][,4]))
###########################################
## Q2 (b)
## plotting
library(ggplot2)
thema<-theme_bw(base_size = 20) +
  theme(axis.title.x = element_text(size = 8, colour = "black"), 
        axis.text.x  = element_text(angle = 0, size = 8, colour = "black"),
        axis.title.y = element_text(size = 8, colour = "black"), 
        axis.text.y  = element_text(angle = 0, size = 8, colour = "black"),
        legend.text  = element_text(size = 8, colour = "black"), 
        legend.title = element_text(size = 8, colour = "black"),
        panel.background = element_rect(fill = "white"),
        panel.grid.major = element_line(colour = "white", linetype = NULL),
        panel.grid.minor = element_line(colour = "white", linetype = NULL),
        text = element_text(size = 8, colour = "black"),
        title =  element_text(size = 8, face = "bold"))

## defining the data frames for plotting
f<-list()
g<-list()
for(j in 1:ncol(start)){
  f[[j]]<-data.frame(M[[j]][[2]],t(M[[j]][[3]]),t(M[[j]][[4]]),j)
  g[[j]]<-data.frame((M[[j]][[1]]),j)
}

f<-do.call(rbind,f)
names(f)<-c("iterations","mean.X1","mean.X2","mean.X3","MCMCse.X1","MCMCse.X2","MCMCse.X3","start.label")

g<-do.call(rbind,g)
names(g)<-c("iterations","samples.X1","samples.X2","samples.X3","start.label")

p<-ggplot(data=f)
q<-ggplot(data=g)
@
Plot of the ACF of samples is given as:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
acf(M[[1]][[1]][,-1],lag.max = 50,main="Plot of ACF of samples")
@
\caption{Plot of ACF of samples}
\end{centering}
\end{figure}
For all the 3 components, the above plot is reasonably good which indicates that the tuning parameter values of $(4.46\times 10^{-6}, 4.33\times 10^{-4}, 2.94\times 10^{-6})$ may not be the best but still it works reasonably good.

For 3 different starting values as (\Sexpr{round(start[1,1],digits=2)},\Sexpr{round(start[2,1],digits=2)},\Sexpr{round(start[3,1],digits=3)}), (\Sexpr{round(start[1,2],digits=2)},\Sexpr{round(start[2,2],digits=2)},\Sexpr{round(start[3,2],digits=3)}) and (\Sexpr{round(start[1,3],digits=2)},\Sexpr{round(start[2,3],digits=2)},\Sexpr{round(start[3,3],digits=3)}), labelled as $1$, $2$ and $3$, plots of estimates of expected values of $(\mu,\xi,\sigma)$ and Monte Carlo standard errors with sample size are given below:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## plotting mean vs. sample size for different starting values
p1<-p+ geom_line(mapping = aes(x=iterations,y=mean.X1,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))+labs(x="Number of samples",y=expression(paste("Estimate of E(", mu ,")")), colour="Label for starting values")+thema+
  geom_errorbar(mapping = aes(x=iterations,y=mean.X1,ymin=mean.X1-MCMCse.X1,ymax=mean.X1+MCMCse.X1,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))
p2<-p+ geom_line(mapping = aes(x=iterations,y=mean.X2,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))+labs(x="Number of samples",y=expression(paste("Estimate of E(", xi ,")")), colour="Label for starting values")+thema+
  geom_errorbar(mapping = aes(x=iterations,y=mean.X2,ymin=mean.X2-MCMCse.X2,ymax=mean.X2+MCMCse.X2,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))
p3<-p+ geom_line(mapping = aes(x=iterations,y=mean.X3,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))+labs(x="Number of samples",y=expression(paste("Estimate of E(", sigma ,")")), colour="Label for starting values")+thema+
  geom_errorbar(mapping = aes(x=iterations,y=mean.X3,ymin=mean.X3-MCMCse.X3,ymax=mean.X3+MCMCse.X3,group=factor(start.label),colour=factor(start.label)),subset=.((iterations%%100)==0))
multiplot(p1, p2, p3,cols=1)
@
\caption{Plot of estimates vs. sample size with error bars for $(\mu,\xi,\sigma)$}
\end{centering}
\end{figure}

\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## plotting (MCMCse) vs. sample size for different starting values
p4<-p+ geom_line(mapping = aes(x=iterations,y=(MCMCse.X1),group=factor(start.label),colour=factor(start.label)))+labs(x="Number of samples",y=expression(paste("MCMCse for", mu)),colour="Label for starting values")+thema
p5<-p+ geom_line(mapping = aes(x=iterations,y=(MCMCse.X2),group=factor(start.label),colour=factor(start.label)))+labs(x="Number of samples",y=expression(paste("MCMCse for", xi)),colour="Label for starting values")+thema
p6<-p+ geom_line(mapping = aes(x=iterations,y=(MCMCse.X3),group=factor(start.label),colour=factor(start.label)))+labs(x="Number of samples",y=expression(paste("MCMCse for", sigma)),colour="Label for starting values")+thema
multiplot(p4, p5, p6,cols=1)
@
\caption{Plot of MCMC se vs. sample size for $(\mu,\xi,\sigma)$}
\end{centering}
\end{figure}
The above plots show that for all the 3 components and for different starting alues, the estimates converge to same value and MCMC standard errors converge to 0. This verifies the algorithm to some extent.
\clearpage
Plotting the estimated density after $n/2$ and after $n$:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## estimated marginal densities after N/2 and after N
q1<-q+ geom_density(mapping = aes(x=samples.X1),fill="steelblue",subset=.(start.label==1&iterations[1:(length(iterations)/2)]))+
  labs(x="",y=expression(paste("For", mu)),title="After N/2")+thema
q2<-q+geom_density(mapping = aes(x=samples.X1),fill="tomato",subset=.(start.label==1))+
  labs(x="",y=expression(paste("For", mu)),title="After N")+thema

q3<-q+ geom_density(mapping = aes(x=samples.X2),fill="steelblue",subset=.(start.label==1&iterations[1:(length(iterations)/2)]))+
  labs(x="",y=expression(paste("For", xi)),title="After N/2")+thema
q4<-q+geom_density(mapping = aes(x=samples.X2),fill="tomato",subset=.(start.label==1))+
  labs(x="",y=expression(paste("For", xi)),title="After N")+thema

q5<-q+ geom_density(mapping = aes(x=samples.X3),fill="steelblue",subset=.(start.label==1&iterations[1:(length(iterations)/2)]))+
  labs(x="",y=expression(paste("For", sigma)),title="After N/2")+thema
q6<-q+geom_density(mapping = aes(x=samples.X3),fill="tomato",subset=.(start.label==1))+
  labs(x="",y=expression(paste("For", sigma)),title="After N")+thema

multiplot(q1,q3,q5,q2,q4,q6, cols=2)
@
\caption{Plot of estimated density for $(\mu,\xi,\sigma)$ after N/2 and N}
\end{centering}
\end{figure}

Plotting the estimated density for different starting values for all the components:
\begin{figure}[H]
\begin{centering}
<<echo=F, fig=TRUE,height=4,width=8>>=
## estimated density for different starting values
q7<-q+ geom_density(mapping = aes(x=samples.X1,group=factor(start.label),colour=factor(start.label)))+
  labs(x="",y=expression(paste("For", mu)),colour="Label of starting values")+thema
q8<-q+ geom_density(mapping = aes(x=samples.X2,group=factor(start.label),colour=factor(start.label)))+
  labs(x="",y=expression(paste("For", xi)),colour="Label of starting values")+thema
q9<-q+ geom_density(mapping = aes(x=samples.X3,group=factor(start.label),colour=factor(start.label)))+
  labs(x="",y=expression(paste("For", sigma)),colour="Label of starting values")+thema
multiplot(q7,q8,q9, cols=1)
@
\caption{Plot of marginal density for $(\mu,\xi,\sigma)$}
\end{centering}
\end{figure}
From the above plots, the estimated densities after $n/2$ and after $n$ look reasonably identical for all the 3 components. Also the estimated density for different starting values also overlap to a good extent. This further verifies the robustness and convergence of the algorithm.
\end{enumerate}

\clearpage
\item The following table gives the required information:
\begin{table}[H]
\centering
\caption{Posterior estimates, standard deviations and MCMC standard errors}
\label{my-label}
\begin{tabular}{|c|c|c|c|}
\hline
            & Posterior estimates for $(\mu,\xi,\sigma)$ & MCMC standard errors & Posterior standard deviations \\ \hline
Algorithm 1 & (\Sexpr{round(M1[[1]][[3]][1,nrow(M1[[1]][[3]])],digits=2)}, \Sexpr{round(M1[[1]][[3]][2,nrow(M1[[1]][[3]])],digits=2)}, \Sexpr{round(M1[[1]][[3]][3,nrow(M1[[1]][[3]])],digits=3)}) &(\Sexpr{round(M1[[1]][[4]][1,nrow(M1[[1]][[4]])],digits=4)}, \Sexpr{round(M1[[1]][[4]][2,nrow(M1[[1]][[4]])],digits=4)}, \Sexpr{round(M1[[1]][[4]][3,nrow(M1[[1]][[4]])],digits=4)})& (\Sexpr{round(mu.se.p1,digits=4)}, \Sexpr{round(zeta.se.p1,digits=4)}, \Sexpr{round(sigma.se.p1,digits=4)})\\ \hline
Algorithm 2 &(\Sexpr{round(M2[[1]][[3]][1,nrow(M2[[1]][[3]])],digits=2)}, \Sexpr{round(M2[[1]][[3]][2,nrow(M2[[1]][[3]])],digits=2)}, \Sexpr{round(M2[[1]][[3]][3,nrow(M2[[1]][[3]])],digits=3)}) &(\Sexpr{round(M2[[1]][[4]][1,nrow(M2[[1]][[4]])],digits=4)}, \Sexpr{round(M2[[1]][[4]][2,nrow(M2[[1]][[4]])],digits=4)}, \Sexpr{round(M2[[1]][[4]][3,nrow(M2[[1]][[4]])],digits=4)})&(\Sexpr{round(mu.se.p2,digits=4)}, \Sexpr{round(zeta.se.p2,digits=4)}, \Sexpr{round(sigma.se.p2,digits=4)})\\ \hline
\end{tabular}
\end{table}

\item The following table gives the required information:

\begin{table}[H]
\centering
\caption{Comparing confidence and credible intervals for different algorithms}
\label{my-label}
\begin{tabular}{|c|c|c|c|}
\hline
               & Conf. int. for $\mu$ & Conf. int. for $\xi$ & Conf. int. for $\sigma$ \\ \hline
BFGS           &(\Sexpr{round(mu.lowc.BFGS,digits=4)}, \Sexpr{round(mu.upc.BFGS,digits=4)})&(\Sexpr{round(zeta.lowc.BFGS,digits=4)}, \Sexpr{round(zeta.upc.BFGS,digits=4)})&(\Sexpr{round(sigma.lowc.BFGS,digits=4)}, \Sexpr{round(sigma.upc.BFGS,digits=4)}) \\ \hline
Nelder-Mead    &(\Sexpr{round(mu.lowc.NM,digits=4)}, \Sexpr{round(mu.upc.NM,digits=4)})  &(\Sexpr{round(zeta.lowc.NM,digits=4)}, \Sexpr{round(zeta.upc.NM,digits=4)})   & (\Sexpr{round(sigma.lowc.NM,digits=4)}, \Sexpr{round(sigma.upc.NM,digits=4)})\\ \hline
Newton Raphson &(\Sexpr{round(mu.lowc.NR,digits=4)}, \Sexpr{round(mu.upc.NR,digits=4)})  &(\Sexpr{round(zeta.lowc.NR,digits=4)}, \Sexpr{round(zeta.upc.NR,digits=4)}) &(\Sexpr{round(sigma.lowc.NR,digits=4)}, \Sexpr{round(sigma.upc.NR,digits=4)})\\ \hline
               &Cred. int. for $\mu$ & Cred. int. for $\xi$ & Cred. int. for $\sigma$ \\ \hline
Algorithm 1    &(\Sexpr{round(mu.lowc.1,digits=4)}, \Sexpr{round(mu.upc.1,digits=4)})&(\Sexpr{round(zeta.lowc.1,digits=4)}, \Sexpr{round(zeta.upc.1,digits=4)})&(\Sexpr{round(sigma.lowc.1,digits=4)}, \Sexpr{round(sigma.upc.1,digits=4)})\\ \hline
Algorithm 2    &(\Sexpr{round(mu.lowc.2,digits=4)}, \Sexpr{round(mu.upc.2,digits=4)})&(\Sexpr{round(zeta.lowc.2,digits=4)}, \Sexpr{round(zeta.upc.2,digits=4)})&(\Sexpr{round(sigma.lowc.2,digits=4)}, \Sexpr{round(sigma.upc.2,digits=4)})\\ \hline
\end{tabular}
\end{table}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Answer 3}
\begin{enumerate}[label=(\alph*)]
\item Among the three optimization algorithms, viz. BFGS, Nelder-Mead and Newton-Raphson, we observed earlier that the Newton Raphson algorithm does not converge if the initial values of the parameters $(\mu,\xi,\sigma)$ are not too close to the MLE's. Also, there is not any apparent difference between BFGS and Nelder-Mead algorithm since the standard errors are exactly same. Therefore best optimization algorithm to compute the MLE's was chosen randomly as Nelder-Mead. With the given parameters $1000$ data points were simulated for one set. The number of sets determine the Monte Carlo sample size.

<<echo=F, results=hide>>=
## Q3 
library(tmvtnorm)
library(fExtremes)
## sourcing batchmeans function used to calculate MCMC standard
## errors later
source("http://www.stat.psu.edu/~mharan/batchmeans.R")

## defining the parameters
{mu<-3
zeta<-0.4
sigma<-0.8}

## log likelihood for calculating MLE from simulated data
log.gev.dens<-function(x,mu,zeta,sigma){
  t4<--log(sigma)
  t1<-(1+zeta*((x-mu)/sigma))
  t2<--(t1^(-1/zeta))
  t3<-(-1-(1/zeta))*log(t1)
  value<-t2+t3+t4
  invisible(value)
}

log.lik<-function(param){
  mu<-param[1]
  zeta<-param[2]
  sigma<-param[3]
  if((sigma>0&&(zeta>0&&(prod(data>(mu-(sigma/zeta)))==1))||(zeta<0&&(prod(data<(mu-(sigma/zeta)))==1))||(zeta==0))){
    t4<--log(sigma)
    value<-sum(log.gev.dens(data,mu = mu,zeta = zeta,sigma = sigma))
  }
  else {value<--(Inf)}
  return(-value)
}

## Monte Carlo sample size chosen based on 5% tolerance for maximum of se for
## mu, zeta and sigma
n<-1000

## initializing the matrix of estimates
p.hat<-matrix(NA_real_,3,n)

## initializing the estimate vectors for all three parameters
mu.hat<-rep(NA_real_,n)
zeta.hat<-rep(NA_real_,n)
sigma.hat<-rep(NA_real_,n)

## initializing the standard errors, MCse and coverage vectors for all three parameters
mu.se<-rep(NA_real_,n)
zeta.se<-rep(NA_real_,n)
sigma.se<-rep(NA_real_,n)
mu.MCse<-rep(NA_real_,n)
zeta.MCse<-rep(NA_real_,n)
sigma.MCse<-rep(NA_real_,n)

mu.cov<-rep(NA_real_,n)
zeta.cov<-rep(NA_real_,n)
sigma.cov<-rep(NA_real_,n)

i<-1
## initializing the grid endpoints to choose intial value for 1st simulation
mu.lower<--10
mu.upper<-10
zeta.lower<--10
zeta.upper<-10
sigma.lower<-0.1
sigma.upper<-10.1


while(i<=n){
  ## simulating 1000 samples from given parameters 
  data<-rgev(n = 1000, xi = zeta, mu = mu, beta = sigma)
  ## evaluating the log likelihood on a grid to choose intial value
  mu.grid<-seq(mu.lower,mu.upper,length.out = 5)
  zeta.grid<-seq(zeta.lower,zeta.upper,length.out = 5)
  sigma.grid<-seq(sigma.lower,sigma.upper,length.out = 5)
  par.grid<-expand.grid(mu.grid,zeta.grid,sigma.grid)
  head(par.grid)
  initial.index<-which.min(apply(X = as.matrix(par.grid),MARGIN = 1,FUN = log.lik))
  initial.value<-par.grid[initial.index,]

  ## estimates
  param.hat<-optim(par = initial.value, fn = log.lik, method = "Nelder-Mead",hessian = TRUE)
  p.hat[,i]<-param.hat$par
  mu.hat[i]<-mean(p.hat[1,1:i])
  zeta.hat[i]<-mean(p.hat[2,1:i])
  sigma.hat[i]<-mean(p.hat[3,1:i])
  
  Info.mat<-param.hat$hessian
  Info.inv<-solve(Info.mat)
  mu.se[i]<-sqrt(Info.inv[1,1])
  mu.MCse[i]<-mu.se[i]/sqrt(i)
  mu.lower<-p.hat[1,i]-(mu.se[i]*qnorm(0.975,mean = 0, sd = 1))
  mu.upper<-p.hat[1,i]+(mu.se[i]*qnorm(0.975,mean = 0, sd = 1))
  mu.cov[i]<-(mu.lower<=mu)&(mu<=mu.upper)
  
  zeta.se[i]<-sqrt(Info.inv[2,2])
  zeta.MCse[i]<-zeta.se[i]/sqrt(i)
  zeta.lower<-p.hat[2,i]-(zeta.se[i]*qnorm(0.975,mean = 0, sd = 1))
  zeta.upper<-p.hat[2,i]+(zeta.se[i]*qnorm(0.975,mean = 0, sd = 1))
  zeta.cov[i]<-(zeta.lower<=zeta)&(zeta<=zeta.upper)
  
  sigma.se[i]<-sqrt(Info.inv[3,3])
  sigma.MCse[i]<-sigma.se[i]/sqrt(i)
  sigma.lower<-p.hat[3,i]-(sigma.se[i]*qnorm(0.975,mean = 0, sd = 1))
  sigma.upper<-p.hat[3,i]+(sigma.se[i]*qnorm(0.975,mean = 0, sd = 1))
  sigma.cov[i]<-(sigma.lower<=sigma)&(sigma<=sigma.upper)
  
  if (max(mu.MCse[i]/mu.hat[i],zeta.MCse[i]/zeta.hat[i],sigma.MCse[i]/sigma.hat[i])<=0.005){
    n.star<-i
    break
  }
  print(max(mu.se[i]/mu.hat[i],zeta.se[i]/zeta.hat[i],sigma.se[i]/sigma.hat[i]))
  i<-i+1
}

############################################
## Q3 (a)

n.star
mu.hat[n.star]
zeta.hat[n.star]
sigma.hat[n.star]

MSE.mu<-mean((p.hat[1,1:n.star]-mu)^2)
MSE.mu
MSE.mu.MCse<-sqrt(var((p.hat[1,1:n.star]-mu)^2)/n.star)
MSE.mu.MCse

MSE.zeta<-mean((p.hat[2,1:n.star]-zeta)^2)
MSE.zeta
MSE.zeta.MCse<-sqrt(var((p.hat[2,1:n.star]-zeta)^2)/n.star)
MSE.zeta.MCse

MSE.sigma<-mean((p.hat[3,1:n.star]-sigma)^2)
MSE.sigma
MSE.sigma.MCse<-sqrt(var((p.hat[3,1:n.star]-sigma)^2)/n.star)
MSE.sigma.MCse

############################################
## Q3 (b)

mu.covp<-sum(mu.cov[1:n.star])/n.star
mu.covp
mu.covp.MCse<-sqrt(var(mu.cov[1:n.star])/n.star)
mu.covp.MCse

zeta.covp<-sum(zeta.cov[1:n.star])/n.star
zeta.covp
zeta.covp.MCse<-sqrt(var(zeta.cov[1:n.star])/n.star)
zeta.covp.MCse

sigma.covp<-sum(sigma.cov[1:n.star])/n.star
sigma.covp
sigma.covp.MCse<-sqrt(var(sigma.cov[1:n.star])/n.star)
sigma.covp.MCse
@

For the three parameters $(\mu,\xi,\sigma)$, the estimates of mean square errors were obtained as (\Sexpr{round(MSE.mu,digits=6)}, \Sexpr{round(MSE.zeta,digits=6)}, \Sexpr{round(MSE.sigma,digits=6)}). The corresponding Monte-Carlo standard errors for these approximations were (\Sexpr{round(MSE.mu.MCse,digits=6)}, \Sexpr{round(MSE.zeta.MCse,digits=6)}, \Sexpr{round(MSE.sigma.MCse,digits=6)}).

\item The coverage probabilties for $95\%$ confidence intervals for $(\mu,\xi,\sigma)$ were obtained as (\Sexpr{round(mu.covp,digits=4)}, \Sexpr{round(zeta.covp,digits=4)}, \Sexpr{round(sigma.covp,digits=4)}). The corresponding Monte-Carlo standard errors for these approximations were (\Sexpr{round(mu.covp.MCse,digits=4)}, \Sexpr{round(zeta.covp.MCse,digits=4)}, \Sexpr{round(sigma.covp.MCse,digits=4)}).

\item The simulation study was carried out by generatng datasets one by one and calculating sample mean estimates and standard for all the three parameters using Nelder Mead algorithm. We define the relative error for each parameter as the ratio of Monte Carlo standard error to the point estimate (sample mean) upto a particular Monte Carlo sample size. The generation of datasets was stopped when the maximum of the relative error for all parameters becomes less than an arbitrary tolerance of $5 \times 10^{-3}$. The number of datasets chosen gives the Monte Carlo sample size of \Sexpr{n.star} for the chosen tolerance. Also at each iterate the log likelihood function was evaluated on a grid of $125$ points in the parameter space. For the first iterate, the grid end-points were initialized arbirarily at $(-10,10)\times (-10,10)\times (0.1,10.1)$. For the successive iterates, the grid end-points were fixed as the $95\%$ confidence bounds obtained in the previous iterate. Note that this approach to choose initial values exploits the information from successive simulated datasets, thereby making the estimates and standard errors more and more accurate as the number of iterations increase.
\end{enumerate}
\end{document}