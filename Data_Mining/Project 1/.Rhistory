variance<-9.4
for (i in 1:10){
n.star[i]<-Expectation.trial(n,start,variance)
}
set.seed(10)
## defining the sample size
n<-50000
## defining the tuning parameter (chosen after many pilot runs)
var.tune<-9.4
## Running the MH algorithm for 3 times with different starting values
start<-c(3.5,3.7,3.9)
## creating a vector of empty lists
result<-vector("list", length(start))
t<-system.time(
for (j in 1:length(start)){
result[[j]]<-Expectation(n,start[j],var.tune)
})
## estimates
result[[1]][[3]][50,1]
result[[2]][[3]][50,1]
result[[3]][[3]][50,1]
## verifying the tuning parameter using acf
acf(result[[1]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
acf(result[[2]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
acf(result[[3]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
## verifying ESS
result[[1]][[6]]/n
result[[2]][[6]]/n
result[[3]][[6]]/n
## verifying ESS rate
result[[1]][[7]]
result[[2]][[7]]
result[[3]][[7]]
## new starting values
result[[1]][[1]][n,2]
result[[2]][[1]][n,2]
result[[3]][[1]][n,2]
##########################################
## Q2 (c)
## plotting
f<-list()
g<-list()
for(j in 1:length(start)){
f[[j]]<-data.frame(result[[j]][[2]],result[[j]][[3]],start[j])
g[[j]]<-data.frame(result[[j]][[1]],start[j])
}
f<-do.call(rbind,f)
names(f)<-c("iterations","mean","MCMCse","start")
g<-do.call(rbind,g)
names(g)<-c("iterations","samples","start")
p<-ggplot(data=f)
q<-ggplot(data=g)
head(g)
thema<-theme_bw(base_size = 20) +
theme(axis.title.x = element_text(size = 8, colour = "black"),
axis.text.x  = element_text(angle = 0, size = 8, colour = "black"),
axis.title.y = element_text(size = 8, colour = "black"),
axis.text.y  = element_text(angle = 0, size = 8, colour = "black"),
legend.text  = element_text(size = 8, colour = "black"),
legend.title = element_text(size = 8, colour = "black"),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour = "white", linetype = NULL),
panel.grid.minor = element_line(colour = "white", linetype = NULL),
text = element_text(size = 8, colour = "black"),
title =  element_text(size = 8, face = "bold"))
## plotting mean vs. sample size for different starting values
p+ geom_line(mapping = aes(x=iterations,y=mean,group=factor(start),colour=factor(start)))+labs(x="Number of samples",y="Estimate of the expectation with MCMC standard errors", colour="Starting values")+thema+
geom_errorbar(mapping = aes(x=iterations,y=mean,ymin=mean-MCMCse,ymax=mean+MCMCse,group=factor(start),colour=factor(start)))
## plotting MCMCse vs. sample size for different starting values
p+ geom_line(mapping = aes(x=iterations,y=MCMCse,group=factor(start),colour=factor(start)))+labs(x="Number of samples",y="MCMC standard errors",colour="Starting values")+thema
set.seed(10)
## defining the sample size
n<-50000
## defining the tuning parameter (chosen after many pilot runs)
var.tune<-9.4
## Running the MH algorithm for 3 times with different starting values
start<-c(3.5,3.7,3.9)
## creating a vector of empty lists
result<-vector("list", length(start))
t<-system.time(
for (j in 1:length(start)){
result[[j]]<-Expectation(n,start[j],var.tune)
})
## estimates
result[[1]][[3]][50,1]
result[[2]][[3]][50,1]
result[[3]][[3]][50,1]
## verifying the tuning parameter using acf
acf(result[[1]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
acf(result[[2]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
acf(result[[3]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
## verifying ESS
result[[1]][[6]]/n
result[[2]][[6]]/n
result[[3]][[6]]/n
## verifying ESS rate
result[[1]][[7]]
result[[2]][[7]]
result[[3]][[7]]
## new starting values
result[[1]][[1]][n,2]
result[[2]][[1]][n,2]
result[[3]][[1]][n,2]
##########################################
## Q2 (c)
## plotting
f<-list()
g<-list()
for(j in 1:length(start)){
f[[j]]<-data.frame(result[[j]][[2]],result[[j]][[3]],start[j])
g[[j]]<-data.frame(result[[j]][[1]],start[j])
}
f<-do.call(rbind,f)
names(f)<-c("iterations","mean","MCMCse","start")
g<-do.call(rbind,g)
names(g)<-c("iterations","samples","start")
p<-ggplot(data=f)
q<-ggplot(data=g)
head(g)
thema<-theme_bw(base_size = 20) +
theme(axis.title.x = element_text(size = 8, colour = "black"),
axis.text.x  = element_text(angle = 0, size = 8, colour = "black"),
axis.title.y = element_text(size = 8, colour = "black"),
axis.text.y  = element_text(angle = 0, size = 8, colour = "black"),
legend.text  = element_text(size = 8, colour = "black"),
legend.title = element_text(size = 8, colour = "black"),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour = "white", linetype = NULL),
panel.grid.minor = element_line(colour = "white", linetype = NULL),
text = element_text(size = 8, colour = "black"),
title =  element_text(size = 8, face = "bold"))
## plotting mean vs. sample size for different starting values
p+ geom_line(mapping = aes(x=iterations,y=mean,group=factor(start),colour=factor(start)))+labs(x="Number of samples",y="Estimate of the expectation with MCMC standard errors", colour="Starting values")+thema+
geom_errorbar(mapping = aes(x=iterations,y=mean,ymin=mean-MCMCse,ymax=mean+MCMCse,group=factor(start),colour=factor(start)))
## estimates
result[[1]][[3]][50,1]
result[[2]][[3]][50,1]
result[[3]][[3]][50,1]
## verifying the tuning parameter using acf
acf(result[[1]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
acf(result[[2]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
acf(result[[3]][[1]][,2],lag.max = 50,main="Plot of ACF of samples")
## verifying ESS
result[[1]][[6]]/n
result[[2]][[6]]/n
result[[3]][[6]]/n
## verifying ESS rate
result[[1]][[7]]
result[[2]][[7]]
result[[3]][[7]]
## new starting values
result[[1]][[1]][n,2]
result[[2]][[1]][n,2]
result[[3]][[1]][n,2]
##########################################
## Q2 (c)
## plotting
f<-list()
g<-list()
for(j in 1:length(start)){
f[[j]]<-data.frame(result[[j]][[2]],result[[j]][[3]],start[j])
g[[j]]<-data.frame(result[[j]][[1]],start[j])
}
f<-do.call(rbind,f)
names(f)<-c("iterations","mean","MCMCse","start")
g<-do.call(rbind,g)
names(g)<-c("iterations","samples","start")
p<-ggplot(data=f)
q<-ggplot(data=g)
head(g)
thema<-theme_bw(base_size = 20) +
theme(axis.title.x = element_text(size = 8, colour = "black"),
axis.text.x  = element_text(angle = 0, size = 8, colour = "black"),
axis.title.y = element_text(size = 8, colour = "black"),
axis.text.y  = element_text(angle = 0, size = 8, colour = "black"),
legend.text  = element_text(size = 8, colour = "black"),
legend.title = element_text(size = 8, colour = "black"),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour = "white", linetype = NULL),
panel.grid.minor = element_line(colour = "white", linetype = NULL),
text = element_text(size = 8, colour = "black"),
title =  element_text(size = 8, face = "bold"))
## plotting mean vs. sample size for different starting values
p+ geom_line(mapping = aes(x=iterations,y=mean,group=factor(start),colour=factor(start)))+labs(x="Number of samples",y="Estimate of the expectation with MCMC standard errors", colour="Starting values")+thema+
geom_errorbar(mapping = aes(x=iterations,y=mean,ymin=mean-MCMCse,ymax=mean+MCMCse,group=factor(start),colour=factor(start)))
## plotting MCMCse vs. sample size for different starting values
p+ geom_line(mapping = aes(x=iterations,y=MCMCse,group=factor(start),colour=factor(start)))+labs(x="Number of samples",y="MCMC standard errors",colour="Starting values")+thema
set.seed(NULL)
install.packages("knitr")
library(mvtnorm)
?dmvtnorm
?dmvnorm
optim(par = c(0,3,4),fn = dmvnorm,lower = c(-1,3,4),upper = c(1,4,5))
?optim
library(MASS)
library(ggplot2)
samp.size <- 10000
n.cov <- 3
n.class <- 3
p.vec <- c(0.3, 0.2, 0.5)
y <- sample(1:n.class, size = samp.size, prob = p.vec, replace = TRUE)
y
y.list <- split(x = y,f = y)
y.list[[1]]
mu <- cbind(c(1,1,1), c(2,2,2), c(3,3,3))
x.list <- list()
for(i in 1:n.class){
x.list[[i]]<- mvrnorm(n = length(y.list[[i]]), mu = mu[,i], Sigma = diag(rep(1, n.cov)) )
}
for(i in 1:n.class){
x.list[[i]]<- mvrnorm(n = length(y.list[[i]]), mu = mu[,i], Sigma = diag(.1,nrow = n.cov) )
}
temp2<-lapply(X = 1:n.class,FUN = function(x){
temp<-data.frame(y.list[[x]],x.list[[x]])
names(temp)<-c("Y","X1","X2","X3")
return(temp)
})
datas<-do.call(what = rbind,args = temp2)
head(datas)
datas <- data.frame(datas)
p<-ggplot(data = datas)
p+geom_point(mapping = aes(x = X1,y = X2,group = factor(Y), colour = factor(Y)))
## Example multinomial logistitic model
library(nnet)
QDA.est<-function(dframe){
n.class<-nlevels(as.factor(dframe[,1]))
n.cov<-ncol(dframe)-1
N<-nrow(dframe)
prior<-rep(NA,n.class)
mk<-matrix(NA,n.cov,n.class)
Sk<-list()
for(i in 1:n.class){
index<-which(as.factor(dframe[,1])==(i-1))
prior[i]<-length(index)/N
mk[,i]<-colMeans(dframe[index,2:(n.cov+1)])
df.centered<-as.matrix(dframe[index,2:(n.cov+1)]-matrix(mk[,i],length(index),n.cov,byrow=TRUE))
Sk[[i]]<-(t(df.centered)%*%df.centered)/(N-n.class)
}
invisible(list("classes"=n.class,"prior"=prior,"mean"=mk,"variance"=Sk))
}
predict.QDA.1<-function(x,n.class,prior,mk,Sk){
del<-rep(NA,n.class)
for (i in 1:n.class){
eig<-eigen(Sk[[i]])
temp<-diag(eig$values^(-1/2),length(eig$values))%*%t(eig$vectors)
temp<-temp%*%cbind(c((x-mk[,i])))
quad.term<-t(temp)%*%temp
del[i]<-((-1/2)*(sum(log(eig$values))+quad.term))+log(prior[i])
}
class<-which.max(del)
invisible(class)
}
predict.QDA.2<-function(x,n.class,prior,mk,Sk){
x<-as.matrix(unlist(x))
del<-rep(NA,n.class)
for (i in 1:n.class){
temp<-ginv(as.matrix(Sk[[i]]))
del[i]<-t(x)%*%temp%*%mk[,i]-((1/2)*t(mk[,i])%*%temp%*%mk[,i])+log(prior[i])
}
QDA.class<-which.max(del)
invisible(QDA.class)
}
QDA.1<-function(dframe,x){
temp1<-QDA.est(dframe)
predict.QDA.1(x,temp1[[1]],temp1[[2]],temp1[[3]],temp1[[4]])
}
QDA.2<-function(dframe,x){
temp1<-QDA.est(dframe)
QDA.class<-predict.QDA.2(x,temp1[[1]],temp1[[2]],temp1[[3]],temp1[[4]])
return(QDA.class-1)
}
data<-read.csv(cv.data.Rdata)
setwd("~/Box Sync/PSU/Fall 2015/Data Mining (STAT 557)/Project 1")
data<-read.csv(cv.data.Rdata)
data<-read.csv(cv_data.Rdata)
data<-read.table(cv_data.Rdata)
data<-load(cv_data.Rdata)
data<-load(cv_data.Rdata)
setwd("~/Box Sync/PSU/Fall 2015/Data Mining (STAT 557)/Project 1")
data<-load(cv_data.Rdata)
data<-load(cv_data.RData)
data<-read.table(cv_data.RData)
data<-load(cv_data.RData)
data<-load("cv_data.RData")
head(data)
data
data<-data.frame(data)
data
data[[1]]
data<-load("cv_data.RData")
data
head(data)
load("cv_data.RData")
head(cv.data)
cv.data$data
cv.data[[1]][[1]]
head(cv.data[[1]][[1]][,1])
head(cv.data[[1]][[1]][,ncol(cv.data[[1]][[1]])])
do.call(cv.data[[1]],rbind)
df<-do.call(rbind,cv.data[[2]])
df
head(df)
head(df)
cv.data[[2]]
df<-do.call(rbind,cv.data[[1]])
head(df)
head(df[,1])
df[,1]<-df$Class2
head(df)
df<-df[,-ncol(df)]
head(df)
QDA.1(df,df[1,])
QDA.1(df,rbind(c(df[1,])))
debug(QDA.est)
QDA.est(df)
n.class
undebug(QDA.est)
t<-QDA.est(df)
t[[3]]
t[[4]][[1]]
solve(t[[4]][[1]])
solve(t[[4]][[2]])
QDA.est<-function(dframe){
n.class<-nlevels(as.factor(dframe[,1]))
n.cov<-ncol(dframe)-1
N<-nrow(dframe)
prior<-rep(NA,n.class)
mk<-matrix(NA,n.cov,n.class)
Sk<-list()
for(i in 1:n.class){
index<-which(as.factor(dframe[,1])==(i-1))
prior[i]<-length(index)/N
mk[,i]<-colMeans(dframe[index,2:(n.cov+1)])
df.centered<-as.matrix(dframe[index,2:(n.cov+1)]-matrix(mk[,i],length(index),n.cov,byrow=TRUE))
Sk[[i]]<-(t(df.centered)%*%df.centered)/(N-n.class)
}
invisible(list("classes"=n.class,"prior"=prior,"mean"=mk,"variance"=Sk))
}
predict.QDA.1<-function(x,n.class,prior,mk,Sk){
del<-rep(NA,n.class)
for (i in 1:n.class){
eig<-eigen(Sk[[i]])
temp<-diag(eig$values^(-1/2),length(eig$values))%*%t(eig$vectors)
temp<-temp%*%cbind(c((x-mk[,i])))
quad.term<-t(temp)%*%temp
del[i]<-((-1/2)*(sum(log(eig$values))+quad.term))+log(prior[i])
}
class<-which.max(del)
invisible(class)
}
QDA.1<-function(dframe,x){
temp1<-QDA.est(dframe)
QDA.class<-predict.QDA.1(x,temp1[[1]],temp1[[2]],temp1[[3]],temp1[[4]])
return(QDA.class-1)
}
QDA.1(df,rbind(c(df[1,])))
rbind(c(df[1,]))
cbind(c(df[1,]))
QDA.1(df,cbind(c(df[1,])))
QDA.1(df,as.vector(cbind(c(df[1,]))))
as.vector(cbind(c(df[1,])))
QDA.1(df,cbind(c(df[1,-1])))
cbind(c(df[1,-1]))
QDA.1(df,cbind(c(df[1,-1])))
length(cbind(c(df[1,-1])))
t
QDA.1(df,cbind(c(df[1,-1])))
debug(QDA.est)
QDA.est(df)
n.class<-nlevels(as.factor(dframe[,1]))
n.cov<-ncol(dframe)-1
N<-nrow(dframe)
prior<-rep(NA,n.class)
mk<-matrix(NA,n.cov,n.class)
View(mk)
t
predict.QDA.1(cbind(c(df[1,-1])),t[[1]],t[[2]],t[[3]],t[[4]])
debug(predict.QDA.1)
predict.QDA.1(cbind(c(df[1,-1])),t[[1]],t[[2]],t[[3]],t[[4]])
View(temp)
mk
x
x-mk[,1]
x-mk[,1]
x
mk[,1]
str(x)
unlist(x)
unlist(x)-mk[,1]
undebug(predict.QDA.1)
str(cbind(c(df[1,-1])))
str(df[1,-1])
c(df[1,-1])
str(c(df[1,-1]))
QDA.1(df,df[1,-1])
undebug(QDA.1)
undebug(predict.QDA.1)
undebug(QDA.est)
QDA.1(df,df[1,-1])
QDA.1(df,as.matrix(df[1,-1]))
QDA.1(df,as.matrix(df[1:2,-1]))
(c.pred-df[,1])^2
c.pred<-rep(NA,nrow(df))
for(i in 1:nrow(df)){
c.pred[i]<-QDA.1(df,as.matrix(df[i,-1]))
}
(c.pred-df[,1])^2
sum((c.pred-df[,1])^2)
# Function: trsfrm.gen
#
# Purpose: Generate transformation functions. Normalization during
# cross validation needs to be based on training data only. So, since
# sample means and sds from the training data will change depending
# on which partition of the data is used for training, different
# transformations will be used for each iteration.
#
# Arguments:
# train.data - training data matrix with column names (should all
#             be numeric).
# Returns:
# trsfrm - function for log transformation and normalization based
# on sample means and sds from trainin data. trsfrm() takes a matrix
# with the same column names passed to trsfrm.gen (can be both
# training and test data)
#
# Example:
# tr.fun <- trsfrm.gen(train.data)
# tr.fun(test.data)
trsfrm.gen <- function(train.data){
ncols <- dim(train.data)[2]
for(i in 1:ncols){
if(names(train.data)[i] %in% c("BrdIndx", "Area",
"Compact", "ShpIndx","SD_NIR", "LW")){
train.data[,i] <- log(train.data[,i])
}
}
train.data <- as.matrix(train.data)
mns <- apply(X = train.data, MARGIN = 2, FUN = mean)
sds <- apply(X = train.data, MARGIN = 2, FUN = mean)
trsfrm <- function(x, mn = mns, sd = sds){
ncols <- dim(x)[2]
for(i in 1:ncols){
if(names(x)[i] %in% c("BrdIndx", "Area",
"Compact", "ShpIndx","SD_NIR", "LW")){
x[,i] <- log(x[,i])
}
}
x <- as.matrix(x)
nrml.data <- matrix(NA, ncol = ncol(x), nrow = nrow(x))
xcols <- dim(x)[2]
for(i in 1:xcols){
nrml.data[,i] <- (x[,i] - mn[i])/sd[i]
}
return(nrml.data)
}
return(trsfrm)
}
sum((c.pred-df[,1])^2)/nrow(df)
{# Function: trsfrm.gen
#
# Purpose: Generate transformation functions. Normalization during
# cross validation needs to be based on training data only. So, since
# sample means and sds from the training data will change depending
# on which partition of the data is used for training, different
# transformations will be used for each iteration.
#
# Arguments:
# train.data - training data matrix with column names (should all
#             be numeric).
# Returns:
# trsfrm - function for log transformation and normalization based
# on sample means and sds from trainin data. trsfrm() takes a matrix
# with the same column names passed to trsfrm.gen (can be both
# training and test data)
#
# Example:
# tr.fun <- trsfrm.gen(train.data)
# tr.fun(test.data)
trsfrm.gen <- function(train.data){
ncols <- dim(train.data)[2]
for(i in 1:ncols){
if(names(train.data)[i] %in% c("BrdIndx", "Area",
"Compact", "ShpIndx","SD_NIR", "LW")){
train.data[,i] <- log(train.data[,i])
}
}
train.data <- as.matrix(train.data)
mns <- apply(X = train.data, MARGIN = 2, FUN = mean)
sds <- apply(X = train.data, MARGIN = 2, FUN = mean)
trsfrm <- function(x, mn = mns, sd = sds){
ncols <- dim(x)[2]
for(i in 1:ncols){
if(names(x)[i] %in% c("BrdIndx", "Area",
"Compact", "ShpIndx","SD_NIR", "LW")){
x[,i] <- log(x[,i])
}
}
x <- as.matrix(x)
nrml.data <- matrix(NA, ncol = ncol(x), nrow = nrow(x))
xcols <- dim(x)[2]
for(i in 1:xcols){
nrml.data[,i] <- (x[,i] - mn[i])/sd[i]
}
return(nrml.data)
}
return(trsfrm)
}}
trsfrm.gen(df)
p<-ggplot(data = df)
p+geom_point(mapping = aes(x = df[,2],y = df[,3],group = factor(df[,1]), colour = factor(df[,1])))
file.choose()
install.packages("shiny")
